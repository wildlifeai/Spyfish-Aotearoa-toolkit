{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module processes csv files stored in AWS and prepares them to load them to powerBI.\n",
    "\n",
    "Functions:\n",
    "    - load the csv files as dataframes.\n",
    "    - process the csv files to have a single annotations dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the right modules installed\n",
    "!pip install pandas boto3 python-dotenv matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS settings\n",
    "dotenv_path = r''\n",
    "# dotenv_path = r'C:\\Users\\YourUsername\\Anaconda3\\envs\\powerbi_env\\.env'\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('s3_data_processing.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file only in local environment\n",
    "if os.environ.get(\"GITHUB_ACTIONS\") != \"true\":\n",
    "    load_dotenv()\n",
    "\n",
    "\n",
    "# S3 configuration\n",
    "def load_aws_credentials(env_path=None):\n",
    "    \"\"\"\n",
    "    Load AWS credentials from .env file.\n",
    "    \n",
    "    Args:\n",
    "        env_path (str, optional): Path to .env file. \n",
    "                                  If None, uses default dotenv behavior.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: AWS access key and secret key\n",
    "    \"\"\"\n",
    "    if env_path:\n",
    "        load_dotenv(env_path)\n",
    "    else:\n",
    "        load_dotenv()\n",
    "    \n",
    "    return (\n",
    "        os.getenv('AWS_ACCESS_KEY_ID'), \n",
    "        os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "        os.getenv('S3_BUCKET')\n",
    "    )\n",
    "\n",
    "\n",
    "def create_s3_client(access_key, secret_key):\n",
    "    \"\"\"\n",
    "    Create and return an S3 client.\n",
    "    \n",
    "    Args:\n",
    "        access_key (str): AWS access key\n",
    "        secret_key (str): AWS secret key\n",
    "    \n",
    "    Returns:\n",
    "        boto3.client: S3 client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return boto3.client(\n",
    "        \"s3\", \n",
    "        aws_access_key_id=access_key, \n",
    "        aws_secret_access_key=secret_key\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create S3 client: {e}\")\n",
    "        raise ValueError(\"Unable to create S3 client. Check AWS credentials.\")\n",
    "\n",
    "\n",
    "def read_csv_from_s3(client, bucket, key):\n",
    "    \"\"\"\n",
    "    Read a CSV file from S3 and return as a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        client (boto3.client): S3 client\n",
    "        bucket (str): S3 bucket name\n",
    "        key (str): S3 object key\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame from CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        return pd.read_csv(obj['Body'], low_memory=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process S3 file {key}: {e}\")\n",
    "        raise IOError(f\"Could not download or read file {key}\")\n",
    "\n",
    "    \n",
    "def process_annotations_dataframe(dataframes: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process annotations dataframe with comprehensive error handling and logging.\n",
    "\n",
    "    Args:\n",
    "        dataframes (Dict[str, pd.DataFrame]): Dictionary of dataframes\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed annotations dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate the max count per movie, annotator and species\n",
    "        max_count_df = (\n",
    "            dataframes[\"annotations\"]\n",
    "            .groupby([\"DropID\", \"annotatedBy\", \"scientificName\"], as_index=False)[\"maxinterval\"]\n",
    "            .max()\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame with all possible combinations\n",
    "        all_combinations = pd.MultiIndex.from_product(\n",
    "            [\n",
    "                max_count_df[\"DropID\"].unique(),\n",
    "                max_count_df[\"annotatedBy\"].unique(),\n",
    "                max_count_df[\"scientificName\"].unique(),\n",
    "            ],\n",
    "            names=[\"DropID\", \"annotatedBy\", \"scientificName\"],\n",
    "        )\n",
    "        all_combinations_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "\n",
    "        # Merge the original DataFrame with all combinations\n",
    "        comb_max_count_df = all_combinations_df.merge(\n",
    "            max_count_df, how=\"left\", on=[\"DropID\", \"annotatedBy\", \"scientificName\"]\n",
    "        )\n",
    "\n",
    "        # Comprehensive merge with additional error handling\n",
    "        annotations_df = (\n",
    "            comb_max_count_df.merge(dataframes[\"movies\"], on=\"DropID\", how=\"left\")\n",
    "            .merge(dataframes[\"sites\"], on=\"SiteID\", how=\"left\")\n",
    "            .merge(dataframes[\"surveys\"], on=\"SurveyID\", how=\"left\")\n",
    "            .merge(dataframes[\"species\"], on=\"scientificName\", how=\"left\")\n",
    "        )\n",
    "\n",
    "        return annotations_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process annotations dataframe: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate S3 file download and processing.\n",
    "    \"\"\"\n",
    "    # CSV file keys\n",
    "    csv_keys = {\n",
    "        \"annotations\": \"spyfish_metadata/kso_csvs/annotations_buv_doc.csv\",\n",
    "        \"movies\": \"spyfish_metadata/kso_csvs/movies_buv_doc.csv\",\n",
    "        \"sites\": \"spyfish_metadata/kso_csvs/sites_buv_doc.csv\",\n",
    "        \"surveys\": \"spyfish_metadata/kso_csvs/surveys_buv_doc.csv\",\n",
    "        \"species\": \"spyfish_metadata/kso_csvs/species_buv_doc.csv\",\n",
    "    }\n",
    "\n",
    "    # Load AWS credentials and create S3 client\n",
    "    access_key, secret_key, bucket_name = load_aws_credentials()\n",
    "    s3_client = create_s3_client(access_key, secret_key)\n",
    "\n",
    "\n",
    "    # Read CSV files from S3 and store in a dictionary of DataFrames\n",
    "    dataframes = {}\n",
    "    for name, key in csv_keys.items():\n",
    "        dataframes[name] = read_csv_from_s3(s3_client, bucket_name, key)\n",
    "\n",
    "    # Calculate the max count per movie, annotator and species\n",
    "    max_count_df = (\n",
    "        dataframes[\"annotations\"]\n",
    "        .groupby([\"DropID\", \"annotatedBy\", \"scientificName\"], as_index=False)[\"maxinterval\"]\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame with all possible combinations of DropID, annotatedBy, and species\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            max_count_df[\"DropID\"].unique(),\n",
    "            max_count_df[\"annotatedBy\"].unique(),\n",
    "            max_count_df[\"scientificName\"].unique(),\n",
    "        ],\n",
    "        names=[\"DropID\", \"annotatedBy\", \"scientificName\"],\n",
    "    )\n",
    "    all_combinations_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "\n",
    "    # Merge the original DataFrame with all combinations, filling missing values with NaN\n",
    "    comb_max_count_df = all_combinations_df.merge(\n",
    "        max_count_df, how=\"left\", on=[\"DropID\", \"annotatedBy\", \"scientificName\"]\n",
    "    )\n",
    "\n",
    "    # Comprehensive merge across different dataframes\n",
    "    annotations_df = (\n",
    "        comb_max_count_df.merge(dataframes[\"movies\"], on=\"DropID\", how=\"left\")\n",
    "        .merge(dataframes[\"sites\"], on=\"SiteID\", how=\"left\")\n",
    "        .merge(dataframes[\"surveys\"], on=\"SurveyID\", how=\"left\")\n",
    "        .merge(dataframes[\"species\"], on=\"scientificName\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Export processed annotations to CSV\n",
    "    annotations_df.to_csv('processed_annotations.csv', index=False)\n",
    "    \n",
    "    return annotations_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
