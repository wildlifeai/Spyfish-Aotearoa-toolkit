{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script looks for GoPro video files in AWS and concatenates them using the \"dropID\" part of the Key as its filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_qqwckw8DDE"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from typing import List, Iterator, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from botocore.exceptions import ClientError\n",
        "import getpass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "from functools import partial\n",
        "import hashlib\n",
        "\n",
        "# Configure logging with a more detailed format\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class AWSCredentials:\n",
        "    access_key_id: str\n",
        "    secret_access_key: str\n",
        "    \n",
        "    @classmethod\n",
        "    def from_user_input(cls) -> 'AWSCredentials':\n",
        "        \"\"\"Securely prompt user for AWS credentials.\"\"\"\n",
        "        access_key = getpass.getpass(\"Enter AWS Access Key ID: \")\n",
        "        secret_key = getpass.getpass(\"Enter AWS Secret Access Key: \")\n",
        "        return cls(access_key, secret_key)\n",
        "\n",
        "class S3Client:\n",
        "    def __init__(self, credentials: Optional[AWSCredentials] = None, max_connections: int = 10):\n",
        "        self.max_connections = max_connections\n",
        "        self.client = self._initialize_client(credentials)\n",
        "\n",
        "    def _initialize_client(self, credentials: Optional[AWSCredentials]) -> boto3.client:\n",
        "        \"\"\"Initialize S3 client with credentials from env vars, provided credentials, or user input.\"\"\"\n",
        "        if credentials is None:\n",
        "            # Try environment variables first\n",
        "            access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "            secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "            \n",
        "            if not access_key or not secret_key:\n",
        "                logger.info(\"AWS credentials not found in environment variables. Please enter them manually.\")\n",
        "                credentials = AWSCredentials.from_user_input()\n",
        "            else:\n",
        "                credentials = AWSCredentials(access_key, secret_key)\n",
        "\n",
        "        try:\n",
        "            # Configure for better performance and reliability\n",
        "            config = boto3.session.Config(\n",
        "                max_pool_connections=self.max_connections,\n",
        "                retries={\n",
        "                    'max_attempts': 5,\n",
        "                    'mode': 'adaptive'\n",
        "                },\n",
        "                connect_timeout=120,\n",
        "                read_timeout=300,\n",
        "                # Add TCP keepalive\n",
        "                tcp_keepalive=True\n",
        "            )\n",
        "            \n",
        "            client = boto3.client(\n",
        "                \"s3\",\n",
        "                aws_access_key_id=credentials.access_key_id,\n",
        "                aws_secret_access_key=credentials.secret_access_key,\n",
        "                config=config\n",
        "            )\n",
        "            # Test the credentials by making a simple API call\n",
        "            client.list_buckets()\n",
        "            logger.info(\"Successfully authenticated with AWS\")\n",
        "            return client\n",
        "        except ClientError as e:\n",
        "            logger.error(\"Failed to authenticate with AWS\")\n",
        "            if \"InvalidAccessKeyId\" in str(e) or \"SignatureDoesNotMatch\" in str(e):\n",
        "                logger.error(\"Invalid credentials provided. Please try again.\")\n",
        "                credentials = AWSCredentials.from_user_input()\n",
        "                return self._initialize_client(credentials)\n",
        "            raise\n",
        "\n",
        "    def list_objects(self, bucket: str, prefix: str = \"\", suffix: str = \"\") -> Iterator[dict]:\n",
        "        \"\"\"List objects in an S3 bucket with optional prefix and suffix filtering.\"\"\"\n",
        "        paginator = self.client.get_paginator(\"list_objects_v2\")\n",
        "        \n",
        "        for prefix_item in [prefix] if isinstance(prefix, str) else prefix:\n",
        "            try:\n",
        "                for page in paginator.paginate(Bucket=bucket, Prefix=prefix_item):\n",
        "                    if \"Contents\" not in page:\n",
        "                        continue\n",
        "                    \n",
        "                    for obj in page[\"Contents\"]:\n",
        "                        if obj[\"Key\"].endswith(suffix):\n",
        "                            yield obj\n",
        "            except ClientError as e:\n",
        "                logger.error(f\"Error listing objects: {e}\")\n",
        "                raise\n",
        "\n",
        "    def download_file_with_retry(self, bucket: str, key: str, filename: Path, max_retries: int = 3) -> bool:\n",
        "        \"\"\"Download a file from S3 with retry logic and integrity checking.\"\"\"\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                logger.info(f\"Downloading {key} (attempt {attempt + 1}/{max_retries})\")\n",
        "                \n",
        "                # Get object metadata first to check expected size\n",
        "                response = self.client.head_object(Bucket=bucket, Key=key)\n",
        "                expected_size = response['ContentLength']\n",
        "                \n",
        "                # Download the file\n",
        "                self.client.download_file(\n",
        "                    Bucket=bucket,\n",
        "                    Key=key,\n",
        "                    Filename=str(filename)\n",
        "                )\n",
        "                \n",
        "                # Verify the download\n",
        "                if filename.exists():\n",
        "                    actual_size = filename.stat().st_size\n",
        "                    if actual_size == expected_size:\n",
        "                        logger.info(f\"‚úÖ Successfully downloaded {key} ({actual_size/1024/1024:.2f} MB)\")\n",
        "                        return True\n",
        "                    else:\n",
        "                        logger.warning(f\"‚ùå Size mismatch for {key}: expected {expected_size}, got {actual_size}\")\n",
        "                        filename.unlink(missing_ok=True)\n",
        "                else:\n",
        "                    logger.warning(f\"‚ùå Downloaded file {filename} does not exist\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                logger.error(f\"‚ùå Download attempt {attempt + 1} failed for {key}: {str(e)}\")\n",
        "                filename.unlink(missing_ok=True)\n",
        "                \n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = 2 ** attempt  # Exponential backoff\n",
        "                    logger.info(f\"Waiting {wait_time} seconds before retry...\")\n",
        "                    time.sleep(wait_time)\n",
        "        \n",
        "        logger.error(f\"‚ùå Failed to download {key} after {max_retries} attempts\")\n",
        "        return False\n",
        "\n",
        "class VideoProcessor:\n",
        "    MOVIE_EXTENSIONS = {'.wmv', '.mpg', '.mov', '.avi', '.mp4', '.MOV', '.MP4'}\n",
        "    \n",
        "    def __init__(self, s3_client: S3Client, bucket: str, max_workers: int = 2):\n",
        "        self.s3_client = s3_client\n",
        "        self.bucket = bucket\n",
        "        self.max_workers = max_workers  # Reduced to prevent connection pool exhaustion\n",
        "        self.download_dir = Path(\"downloaded_movies\")\n",
        "        self.output_dir = Path(\"concatenated_videos\")\n",
        "        \n",
        "        # Create necessary directories\n",
        "        self.download_dir.mkdir(exist_ok=True)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Find and verify ffmpeg\n",
        "        self.ffmpeg_path = self._find_ffmpeg()\n",
        "        if not self.ffmpeg_path:\n",
        "            raise RuntimeError(\n",
        "                \"ffmpeg not found. Please install ffmpeg:\\n\"\n",
        "                \"1. Download from https://github.com/BtbN/FFmpeg-Builds/releases\\n\"\n",
        "                \"2. Extract the zip file\\n\"\n",
        "                \"3. Add the bin folder to your system PATH or place ffmpeg.exe in your working directory\"\n",
        "            )\n",
        "\n",
        "    def _find_ffmpeg(self) -> Optional[str]:\n",
        "        \"\"\"Find ffmpeg executable in various locations.\"\"\"\n",
        "        try:\n",
        "            # Check if ffmpeg is in PATH\n",
        "            result = subprocess.run(['ffmpeg', '-version'], \n",
        "                                 capture_output=True, \n",
        "                                 check=False)\n",
        "            if result.returncode == 0:\n",
        "                return 'ffmpeg'\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "        # Check common Windows locations\n",
        "        possible_paths = [\n",
        "            Path.cwd() / \"ffmpeg.exe\",  # Current directory\n",
        "            Path.cwd() / \"bin\" / \"ffmpeg.exe\",  # bin subdirectory\n",
        "            Path(os.getenv('PROGRAMFILES', '')) / \"ffmpeg\" / \"bin\" / \"ffmpeg.exe\",\n",
        "            Path(os.getenv('PROGRAMFILES(X86)', '')) / \"ffmpeg\" / \"bin\" / \"ffmpeg.exe\",\n",
        "        ]\n",
        "\n",
        "        # Add conda environment path if running in conda\n",
        "        conda_prefix = os.getenv('CONDA_PREFIX')\n",
        "        if conda_prefix:\n",
        "            possible_paths.append(Path(conda_prefix) / \"Library\" / \"bin\" / \"ffmpeg.exe\")\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                logger.info(f\"Found ffmpeg at: {path}\")\n",
        "                return str(path)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def verify_video_file(self, file_path: Path) -> bool:\n",
        "        \"\"\"Verify that a video file exists and has non-zero size.\"\"\"\n",
        "        try:\n",
        "            if not file_path.exists():\n",
        "                logger.error(f\"Video file does not exist: {file_path}\")\n",
        "                return False\n",
        "            \n",
        "            size = file_path.stat().st_size\n",
        "            if size == 0:\n",
        "                logger.error(f\"Video file is empty: {file_path}\")\n",
        "                return False\n",
        "                \n",
        "            logger.info(f\"Verified video file: {file_path} (size: {size/1024/1024:.2f} MB)\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error verifying video file {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def verify_video_file_deep(self, file_path: Path) -> bool:\n",
        "        \"\"\"Deep verification using ffmpeg probe.\"\"\"\n",
        "        try:\n",
        "            # Try to read video metadata using ffprobe (more lightweight than ffmpeg)\n",
        "            cmd = [\n",
        "                'ffprobe',\n",
        "                '-v', 'quiet',\n",
        "                '-print_format', 'json',\n",
        "                '-show_format',\n",
        "                '-show_streams',\n",
        "                str(file_path)\n",
        "            ]\n",
        "            \n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n",
        "            if result.returncode != 0:\n",
        "                logger.error(f\"ffprobe failed for {file_path}: {result.stderr}\")\n",
        "                return False\n",
        "            \n",
        "            # Check if we got valid JSON output\n",
        "            import json\n",
        "            try:\n",
        "                data = json.loads(result.stdout)\n",
        "                if 'format' in data and 'streams' in data:\n",
        "                    logger.info(f\"‚úÖ Video file verified: {file_path}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    logger.error(f\"Invalid video format for {file_path}\")\n",
        "                    return False\n",
        "            except json.JSONDecodeError:\n",
        "                logger.error(f\"Invalid ffprobe output for {file_path}\")\n",
        "                return False\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            logger.error(f\"ffprobe timeout for {file_path}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error verifying video file {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def concatenate_videos(self, video_paths: List[Path], output_path: Path, verify_videos: bool) -> bool:\n",
        "        \"\"\"Concatenate multiple videos using ffmpeg with hardware acceleration if available.\"\"\"\n",
        "        list_file = None\n",
        "        try:\n",
        "            if verify_videos:\n",
        "                # Verify all input files exist and are valid\n",
        "                logger.info(f\"Verifying {len(video_paths)} input videos...\")\n",
        "                valid_videos = []\n",
        "                for path in video_paths:\n",
        "                    if self.verify_video_file_deep(path):\n",
        "                        valid_videos.append(path)\n",
        "                    else:\n",
        "                        logger.error(f\"Skipping invalid video: {path}\")\n",
        "                        \n",
        "                if not valid_videos:\n",
        "                    raise ValueError(\"No valid videos found to concatenate\")\n",
        "                \n",
        "                if len(valid_videos) != len(video_paths):\n",
        "                    logger.warning(f\"Only {len(valid_videos)} out of {len(video_paths)} videos are valid\")\n",
        "            else:\n",
        "                valid_videos = video_paths\n",
        "                \n",
        "            total_input_size = sum(path.stat().st_size for path in valid_videos)\n",
        "            logger.info(f\"Total input size: {total_input_size/1024/1024:.2f} MB\")\n",
        "            \n",
        "            # Create a temporary file list for ffmpeg\n",
        "            list_file = self.download_dir / \"file_list.txt\"\n",
        "            with open(list_file, 'w', encoding='utf-8') as f:\n",
        "                for path in valid_videos:\n",
        "                    # Use forward slashes for cross-platform compatibility\n",
        "                    f.write(f\"file '{str(path.absolute()).replace(chr(92), '/')}'\\n\")\n",
        "            \n",
        "            logger.info(f\"Created concat list file at {list_file}\")\n",
        "            \n",
        "            # Build ffmpeg command with hardware acceleration\n",
        "            base_cmd = [self.ffmpeg_path, '-y', '-f', 'concat', '-safe', '0', '-i', str(list_file)]\n",
        "            \n",
        "            logger.info(\"üêå Using CPU (-c copy) for concatenation.\")\n",
        "            codec_cmd = ['-c', 'copy']\n",
        "            \n",
        "            cmd = base_cmd + codec_cmd + [str(output_path)]\n",
        "    \n",
        "            logger.info(f\"Running command: {' '.join(cmd)}\")\n",
        "            \n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                result = subprocess.run(\n",
        "                    cmd, capture_output=True, text=True, check=True, encoding='utf-8', timeout=3600\n",
        "                )\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                logger.info(f\"‚úÖ Success! Concatenation took {elapsed_time:.2f} seconds.\")\n",
        "                \n",
        "                # Verify the output file\n",
        "                if not output_path.exists() or output_path.stat().st_size == 0:\n",
        "                    logger.error(\"Output video file is missing or empty\")\n",
        "                    return False\n",
        "                \n",
        "                output_size = output_path.stat().st_size\n",
        "                logger.info(f\"Output file size: {output_size/1024/1024:.2f} MB\")\n",
        "                return True\n",
        "                \n",
        "            except subprocess.CalledProcessError as e:\n",
        "                logger.error(\"‚ùå FFmpeg command failed!\")\n",
        "                logger.error(f\"Stderr:\\n{e.stderr}\")\n",
        "                return False\n",
        "            except subprocess.TimeoutExpired:\n",
        "                logger.error(\"‚ùå FFmpeg command timed out!\")\n",
        "                return False\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Concatenation failed: {str(e)}\")\n",
        "            return False\n",
        "        finally:\n",
        "            # Clean up the temporary file list\n",
        "            if list_file and list_file.exists():\n",
        "                try:\n",
        "                    list_file.unlink()\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error cleaning up list file: {str(e)}\")\n",
        "\n",
        "    def _download_single_video(self, key: str, progress_dict: dict = None) -> Path:\n",
        "        \"\"\"Download a single video file with retry logic.\"\"\"\n",
        "        local_path = self.download_dir / Path(key).name\n",
        "        try:\n",
        "            success = self.s3_client.download_file_with_retry(self.bucket, key, local_path)\n",
        "            if progress_dict is not None:\n",
        "                progress_dict[key] = success\n",
        "            if success:\n",
        "                return local_path\n",
        "            else:\n",
        "                raise RuntimeError(f\"Failed to download {key}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to download {key}: {str(e)}\")\n",
        "            if progress_dict is not None:\n",
        "                progress_dict[key] = False\n",
        "            raise\n",
        "\n",
        "    def _download_videos_sequential(self, keys: pd.Series) -> List[Path]:\n",
        "        \"\"\"Download all videos for a drop sequentially (more reliable for large files).\"\"\"\n",
        "        downloaded_files = []\n",
        "        \n",
        "        for key in keys:\n",
        "            try:\n",
        "                local_path = self._download_single_video(key)\n",
        "                downloaded_files.append(local_path)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Download failed for {key}: {str(e)}\")\n",
        "                # Continue with other files\n",
        "                continue\n",
        "        \n",
        "        # Sort files by name to ensure correct order\n",
        "        downloaded_files.sort()\n",
        "        return downloaded_files\n",
        "\n",
        "    def _download_videos_parallel(self, keys: pd.Series) -> List[Path]:\n",
        "        \"\"\"Download all videos for a drop in parallel.\"\"\"\n",
        "        downloaded_files = []\n",
        "        progress_dict = {}\n",
        "        \n",
        "        # Use ThreadPoolExecutor for parallel downloads\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Submit all download tasks\n",
        "            future_to_key = {\n",
        "                executor.submit(self._download_single_video, key, progress_dict): key \n",
        "                for key in keys\n",
        "            }\n",
        "            \n",
        "            # Process completed downloads\n",
        "            for future in as_completed(future_to_key):\n",
        "                key = future_to_key[future]\n",
        "                try:\n",
        "                    local_path = future.result()\n",
        "                    downloaded_files.append(local_path)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Download failed for {key}: {str(e)}\")\n",
        "        \n",
        "        # Sort files by name to ensure correct order\n",
        "        downloaded_files.sort()\n",
        "        return downloaded_files\n",
        "\n",
        "    def _process_single_drop(self, drop_data: pd.DataFrame, delete_originals: bool, test_mode: bool, verify_videos: bool, sequential_download: bool = True) -> None:\n",
        "        \"\"\"Process a single drop's worth of videos.\"\"\"\n",
        "        downloaded_files = []\n",
        "        output_path = None\n",
        "        \n",
        "        try:\n",
        "            # Download files (sequential is more reliable for large files)\n",
        "            if sequential_download:\n",
        "                downloaded_files = self._download_videos_sequential(drop_data['Key'])\n",
        "            else:\n",
        "                downloaded_files = self._download_videos_parallel(drop_data['Key'])\n",
        "            \n",
        "            if not downloaded_files:\n",
        "                raise RuntimeError(\"No files were successfully downloaded\")\n",
        "            \n",
        "            logger.info(f\"Processing files in order: {[f.name for f in downloaded_files]}\")\n",
        "            \n",
        "            if verify_videos:\n",
        "                # Quick verification of downloaded files\n",
        "                valid_files = []\n",
        "                for file_path in downloaded_files:\n",
        "                    if self.verify_video_file(file_path):\n",
        "                        valid_files.append(file_path)\n",
        "                    else:\n",
        "                        logger.error(f\"Downloaded file is corrupted: {file_path}\")\n",
        "                \n",
        "                if not valid_files:\n",
        "                    raise RuntimeError(\"No valid video files available for processing\")\n",
        "            else:\n",
        "                valid_files = downloaded_files            \n",
        "            \n",
        "            output_path = self._concatenate_drop_videos(valid_files, drop_data['DropID'].iloc[0], verify_videos)\n",
        "            \n",
        "            if not test_mode:\n",
        "                self._upload_and_cleanup(output_path, drop_data, delete_originals)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing drop: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            # Ensure cleanup happens even if there's an error\n",
        "            self._cleanup_files(downloaded_files, output_path)\n",
        "\n",
        "    def process_gopro_videos(\n",
        "        self,\n",
        "        filtered_df: pd.DataFrame,\n",
        "        delete_originals: bool = False,\n",
        "        test_mode: bool = False,\n",
        "        gopro_prefix: str = \"GX\",\n",
        "        verify_videos: bool = False,\n",
        "        parallel_drops: bool = False,\n",
        "        sequential_download: bool = True\n",
        "    ) -> None:\n",
        "        \"\"\"Process GoPro videos by DropID with optional parallel processing.\"\"\"\n",
        "        \n",
        "        drop_ids = filtered_df['DropID'].unique()\n",
        "        valid_drops = []\n",
        "        \n",
        "        # Filter valid drops\n",
        "        for drop_id in drop_ids:\n",
        "            drop_data = filtered_df[filtered_df['DropID'] == drop_id]\n",
        "            if all(str(name).startswith(gopro_prefix) for name in drop_data['fileName']):\n",
        "                valid_drops.append((drop_id, drop_data))\n",
        "            else:\n",
        "                logger.warning(f\"Skipping DropID {drop_id}: Not all videos start with {gopro_prefix}\")\n",
        "        \n",
        "        logger.info(f\"Processing {len(valid_drops)} valid drops\")\n",
        "        \n",
        "        if parallel_drops and len(valid_drops) > 1:\n",
        "            # Process drops in parallel (careful with resource usage)\n",
        "            with ThreadPoolExecutor(max_workers=min(2, len(valid_drops))) as executor:\n",
        "                futures = []\n",
        "                for drop_id, drop_data in valid_drops:\n",
        "                    future = executor.submit(\n",
        "                        self._process_single_drop, \n",
        "                        drop_data, delete_originals, test_mode, verify_videos, sequential_download\n",
        "                    )\n",
        "                    futures.append((drop_id, future))\n",
        "                \n",
        "                for drop_id, future in futures:\n",
        "                    try:\n",
        "                        future.result()\n",
        "                        logger.info(f\"‚úÖ Successfully processed DropID {drop_id}\")\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"‚ùå Error processing DropID {drop_id}: {str(e)}\")\n",
        "        else:\n",
        "            # Sequential processing\n",
        "            for drop_id, drop_data in valid_drops:\n",
        "                try:\n",
        "                    logger.info(f\"Processing DropID {drop_id}...\")\n",
        "                    self._process_single_drop(drop_data, delete_originals, test_mode, verify_videos, sequential_download)\n",
        "                    logger.info(f\"‚úÖ Successfully processed DropID {drop_id}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"‚ùå Error processing DropID {drop_id}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    def get_movies_df(self, prefix: str = \"\") -> pd.DataFrame:\n",
        "        \"\"\"Get DataFrame of movie files in S3 bucket with their sizes.    \n",
        "        Args:\n",
        "            prefix: Optional prefix to filter S3 objects            \n",
        "        Returns:\n",
        "            DataFrame with columns 'Key' and 'Size' (in bytes)\n",
        "        \"\"\"\n",
        "        # Get all objects matching the prefix and movie extensions\n",
        "        objects = self.s3_client.list_objects(\n",
        "            self.bucket,\n",
        "            prefix=prefix,\n",
        "            suffix=tuple(self.MOVIE_EXTENSIONS)\n",
        "        )\n",
        "        \n",
        "        # Extract both keys and sizes\n",
        "        movie_data = [\n",
        "            {\n",
        "                'Key': obj['Key'],\n",
        "                'Size': obj['Size']  # Size in bytes\n",
        "            }\n",
        "            for obj in objects\n",
        "        ]\n",
        "        \n",
        "        return pd.DataFrame(movie_data)\n",
        "\n",
        "    def _concatenate_drop_videos(self, video_paths: List[Path], drop_id: str, verify_videos: bool) -> Path:\n",
        "        \"\"\"Concatenate videos for a single drop.\"\"\"\n",
        "        output_path = self.output_dir / f\"{drop_id}.mp4\"\n",
        "        if not self.concatenate_videos(video_paths, output_path, verify_videos):\n",
        "            raise RuntimeError(\"Video concatenation failed\")\n",
        "        return output_path\n",
        "\n",
        "    def _upload_and_cleanup(self, output_path: Path, drop_data: pd.DataFrame, delete_originals: bool) -> None:\n",
        "        \"\"\"Upload concatenated video and cleanup originals if requested.\"\"\"\n",
        "        new_key = f\"{drop_data['SurveyID'].iloc[0]}/{drop_data['DropID'].iloc[0]}/{drop_data['DropID'].iloc[0]}.mp4\"\n",
        "        \n",
        "        try:            \n",
        "            self.s3_client.client.upload_file(\n",
        "                str(output_path),\n",
        "                self.bucket,\n",
        "                new_key\n",
        "            )\n",
        "                    \n",
        "            logger.info(f\"Successfully uploaded concatenated video to {new_key}\")\n",
        "            \n",
        "            # Delete original files if requested\n",
        "            if delete_originals:\n",
        "                for key in drop_data['Key']:\n",
        "                    self.s3_client.client.delete_object(Bucket=self.bucket, Key=key)\n",
        "                    logger.info(f\"Deleted original file {key}\")\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during upload of {new_key}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    @staticmethod\n",
        "    def _cleanup_files(downloaded_files: List[Path], output_path: Optional[Path]) -> None:\n",
        "        \"\"\"Clean up local files.\"\"\"\n",
        "        for file_path in downloaded_files:\n",
        "            if file_path.exists():\n",
        "                try:\n",
        "                    file_path.unlink()\n",
        "                    logger.info(f\"Cleaned up {file_path}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error deleting {file_path}: {str(e)}\")\n",
        "        \n",
        "        if output_path and output_path.exists():\n",
        "            try:\n",
        "                output_path.unlink()\n",
        "                logger.info(f\"Cleaned up {output_path}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error deleting {output_path}: {str(e)}\")\n",
        "\n",
        "\n",
        "def get_filtered_movies_df(movies_df: pd.DataFrame, gopro_ext: str = \"BNP\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filter movies DataFrame to remove DropID groups where a file has the same name as its DropID.\n",
        "    \n",
        "    Args:\n",
        "        movies_df: DataFrame with Key column containing file paths\n",
        "        gopro_ext: Prefix used to identify GoPro files\n",
        "    \n",
        "    Returns:\n",
        "        Filtered DataFrame containing only valid GoPro groups\n",
        "    \"\"\"\n",
        "    # Create the SurveyID, DropID, and fileName columns from the Key\n",
        "    df = movies_df.assign(\n",
        "        SurveyID=movies_df['Key'].str.split('/', expand=True)[0],\n",
        "        DropID=movies_df['Key'].str.split('/', expand=True)[1],\n",
        "        fileName=movies_df['Key'].str.split('/', expand=True)[2]\n",
        "    )\n",
        "    \n",
        "    # Filter for GoPro movies\n",
        "    go_pro_movies_df = df[df.fileName.str.startswith(gopro_ext)].copy()\n",
        "    \n",
        "    # Remove .mp4 extension from fileName for comparison\n",
        "    go_pro_movies_df['fileNameNoExt'] = go_pro_movies_df['fileName'].str.replace('.mp4', '')\n",
        "    \n",
        "    # Find DropIDs where any fileName (without extension) matches the DropID\n",
        "    matching_dropids = go_pro_movies_df[\n",
        "        go_pro_movies_df.apply(\n",
        "            lambda row: row['fileNameNoExt'] == row['DropID'], \n",
        "            axis=1\n",
        "        )\n",
        "    ]['DropID'].unique()\n",
        "    \n",
        "    # Remove groups where DropID matches any fileName\n",
        "    df_no_matching = go_pro_movies_df[~go_pro_movies_df['DropID'].isin(matching_dropids)]\n",
        "    \n",
        "    # Group by 'DropID' and count unique 'fileName' values for each group\n",
        "    grouped_counts = df_no_matching.groupby('DropID')['fileName'].nunique()\n",
        "    \n",
        "    # Filter for 'DropID's with more than one unique 'fileName'\n",
        "    filtered_dropids = grouped_counts[grouped_counts > 1].index\n",
        "    \n",
        "    # Filter the DataFrame to retain only rows with the filtered 'DropID's\n",
        "    filtered_df = df_no_matching[df_no_matching['DropID'].isin(filtered_dropids)]\n",
        "    \n",
        "    # Drop the temporary fileNameNoExt column\n",
        "    filtered_df = filtered_df.drop('fileNameNoExt', axis=1)\n",
        "    \n",
        "    return filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbJb13_D1AhN"
      },
      "source": [
        "# Get info from go pro movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIqe28PQ072M"
      },
      "outputs": [],
      "source": [
        "# Initialize with better connection settings\n",
        "s3_client = S3Client(max_connections=10)\n",
        "\n",
        "# Create video processor with conservative settings\n",
        "processor = VideoProcessor(s3_client, bucket=\"marine-buv\", max_workers=2)\n",
        "\n",
        "# Get movies\n",
        "movies_df = processor.get_movies_df(prefix=\"AHE\")\n",
        "gopro_ext = \"GX\"\n",
        "filtered_df = get_filtered_movies_df(movies_df=movies_df, gopro_ext=gopro_ext)\n",
        "filtered_df\n",
        "# Process with sequential downloads (more reliable)\n",
        "processor.process_gopro_videos(\n",
        "    filtered_df=filtered_df,\n",
        "    delete_originals=False,\n",
        "    test_mode=False,\n",
        "    gopro_prefix=gopro_ext,\n",
        "    verify_videos=True,  # Enable verification to catch corrupted files\n",
        "    parallel_drops=False,  # Process one drop at a time\n",
        "    sequential_download=True  # Download files one at a time (most reliable)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_already_concatenated_movies_df(movies_df: pd.DataFrame, size_tolerance: float = 0.05) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Find individual video files that should be removed because a concatenated file already exists.\n",
        "    \n",
        "    This function groups files by DropID, orders by size (largest first), and checks if the largest file\n",
        "    has approximately the same size as the sum of all other files. If so, it assumes the largest file\n",
        "    is the concatenated version and returns the smaller files for removal.\n",
        "    \n",
        "    Args:\n",
        "        movies_df: DataFrame with Key column containing file paths and Size column\n",
        "        size_tolerance: Tolerance for size comparison (default 0.05 = 5%)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame containing only individual files that should be removed (where concatenated version exists)\n",
        "    \"\"\"\n",
        "    # Create the SurveyID, DropID, and fileName columns from the Key\n",
        "    df = movies_df.assign(\n",
        "        SurveyID=movies_df['Key'].str.split('/', expand=True)[0],\n",
        "        DropID=movies_df['Key'].str.split('/', expand=True)[1],\n",
        "        fileName=movies_df['Key'].str.split('/', expand=True)[2]\n",
        "    )\n",
        "    \n",
        "    # Only consider DropIDs with multiple files\n",
        "    dropid_counts = df.groupby('DropID').size()\n",
        "    multi_file_dropids = dropid_counts[dropid_counts > 1].index\n",
        "    \n",
        "    print(f\"Found {len(multi_file_dropids)} DropIDs with multiple files\")\n",
        "    \n",
        "    files_to_remove = []\n",
        "    concatenated_found = 0\n",
        "    \n",
        "    for drop_id in multi_file_dropids:\n",
        "        drop_files = df[df['DropID'] == drop_id].copy()\n",
        "        \n",
        "        # Sort by size (largest first)\n",
        "        drop_files = drop_files.sort_values('Size', ascending=False)\n",
        "        \n",
        "        if len(drop_files) < 2:\n",
        "            continue\n",
        "            \n",
        "        largest_file = drop_files.iloc[0]\n",
        "        other_files = drop_files.iloc[1:]\n",
        "        \n",
        "        largest_size = largest_file['Size']\n",
        "        sum_others_size = other_files['Size'].sum()\n",
        "        \n",
        "        # Check if largest file size is approximately equal to sum of others\n",
        "        # Allow for some tolerance due to encoding differences, metadata, etc.\n",
        "        size_ratio = abs(largest_size - sum_others_size) / max(largest_size, sum_others_size)\n",
        "        \n",
        "        if size_ratio <= size_tolerance:\n",
        "            # Largest file is likely concatenated version of the others\n",
        "            files_to_remove.extend(other_files.to_dict('records'))\n",
        "            concatenated_found += 1\n",
        "            \n",
        "            print(f\"DropID {drop_id}: Concatenated file ({largest_size/1024/1024:.1f}MB) ‚âà Sum of {len(other_files)} files ({sum_others_size/1024/1024:.1f}MB)\")\n",
        "        else:\n",
        "            print(f\"DropID {drop_id}: Size mismatch - Largest: {largest_size/1024/1024:.1f}MB, Sum others: {sum_others_size/1024/1024:.1f}MB (ratio: {size_ratio:.3f})\")\n",
        "    \n",
        "    if files_to_remove:\n",
        "        result_df = pd.DataFrame(files_to_remove)\n",
        "        total_size_gb = result_df['Size'].sum() / (1024**3)\n",
        "        \n",
        "        print(f\"\\nFound {concatenated_found} DropIDs with concatenated files\")\n",
        "        print(f\"Total files to remove: {len(result_df)}\")\n",
        "        print(f\"Total size to be removed: {total_size_gb:.2f} GB\")\n",
        "        \n",
        "        # Show examples\n",
        "        print(\"\\nExample DropIDs with files to remove:\")\n",
        "        for drop_id in result_df['DropID'].unique()[:3]:\n",
        "            files = result_df[result_df['DropID'] == drop_id]['fileName'].tolist()\n",
        "            sizes = result_df[result_df['DropID'] == drop_id]['Size'].tolist()\n",
        "            print(f\"  {drop_id}: {len(files)} files, {sum(sizes)/1024/1024:.1f}MB total\")\n",
        "            for file, size in zip(files[:3], sizes[:3]):  # Show first 3 files\n",
        "                print(f\"    - {file} ({size/1024/1024:.1f}MB)\")\n",
        "        \n",
        "        return result_df\n",
        "    else:\n",
        "        print(\"No files found for removal (no concatenated files detected)\")\n",
        "        return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get individual files that can be removed (concatenated version exists)\n",
        "files_to_remove = find_already_concatenated_movies_df(movies_df, size_tolerance=0.01)\n",
        "\n",
        "\n",
        "# Remove redundant files\n",
        "for _, row in files_to_remove.iterrows():\n",
        "    print(f\"Safe to remove: {row['Key']} ({row['Size']/1024/1024:.1f}MB)\")\n",
        "    # s3_client.delete_object(Bucket=bucket, Key=row['Key'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPWNn5s5FtwbB065Ta3vhnh",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
