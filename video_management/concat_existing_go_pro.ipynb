{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script looks for GoPro video files in AWS and concatenates them using the \"dropID\" part of the Key as its filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_qqwckw8DDE"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install ffmpeg if not installed already\n",
        "# !conda install ffmpeg -c conda-forge -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import boto3\n",
        "import ffmpeg\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Iterator, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from botocore.exceptions import ClientError\n",
        "from tqdm import tqdm\n",
        "import getpass\n",
        "\n",
        "# Configure logging with a more detailed format\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class AWSCredentials:\n",
        "    access_key_id: str\n",
        "    secret_access_key: str\n",
        "    \n",
        "    @classmethod\n",
        "    def from_user_input(cls) -> 'AWSCredentials':\n",
        "        \"\"\"Securely prompt user for AWS credentials.\"\"\"\n",
        "        access_key = getpass.getpass(\"Enter AWS Access Key ID: \")\n",
        "        secret_key = getpass.getpass(\"Enter AWS Secret Access Key: \")\n",
        "        return cls(access_key, secret_key)\n",
        "\n",
        "class S3Client:\n",
        "    def __init__(self, credentials: Optional[AWSCredentials] = None):\n",
        "        self.client = self._initialize_client(credentials)\n",
        "\n",
        "    def _initialize_client(self, credentials: Optional[AWSCredentials]) -> boto3.client:\n",
        "        \"\"\"Initialize S3 client with credentials from env vars, provided credentials, or user input.\"\"\"\n",
        "        if credentials is None:\n",
        "            # Try environment variables first\n",
        "            access_key = os.getenv(\"SPY_KEY\")\n",
        "            secret_key = os.getenv(\"SPY_SECRET\")\n",
        "            \n",
        "            if not access_key or not secret_key:\n",
        "                logger.info(\"AWS credentials not found in environment variables. Please enter them manually.\")\n",
        "                credentials = AWSCredentials.from_user_input()\n",
        "            else:\n",
        "                credentials = AWSCredentials(access_key, secret_key)\n",
        "\n",
        "        try:\n",
        "            client = boto3.client(\n",
        "                \"s3\",\n",
        "                aws_access_key_id=credentials.access_key_id,\n",
        "                aws_secret_access_key=credentials.secret_access_key,\n",
        "            )\n",
        "            # Test the credentials by making a simple API call\n",
        "            client.list_buckets()\n",
        "            logger.info(\"Successfully authenticated with AWS\")\n",
        "            return client\n",
        "        except ClientError as e:\n",
        "            logger.error(\"Failed to authenticate with AWS\")\n",
        "            if \"InvalidAccessKeyId\" in str(e) or \"SignatureDoesNotMatch\" in str(e):\n",
        "                logger.error(\"Invalid credentials provided. Please try again.\")\n",
        "                credentials = AWSCredentials.from_user_input()\n",
        "                return self._initialize_client(credentials)\n",
        "            raise\n",
        "\n",
        "    def list_objects(self, bucket: str, prefix: str = \"\", suffix: str = \"\") -> Iterator[dict]:\n",
        "        \"\"\"List objects in an S3 bucket with optional prefix and suffix filtering.\"\"\"\n",
        "        paginator = self.client.get_paginator(\"list_objects_v2\")\n",
        "        \n",
        "        for prefix_item in [prefix] if isinstance(prefix, str) else prefix:\n",
        "            try:\n",
        "                for page in paginator.paginate(Bucket=bucket, Prefix=prefix_item):\n",
        "                    if \"Contents\" not in page:\n",
        "                        continue\n",
        "                    \n",
        "                    for obj in page[\"Contents\"]:\n",
        "                        if obj[\"Key\"].endswith(suffix):\n",
        "                            yield obj\n",
        "            except ClientError as e:\n",
        "                logger.error(f\"Error listing objects: {e}\")\n",
        "                raise\n",
        "\n",
        "    def download_file(self, bucket: str, key: str, filename: Path, version_id: Optional[str] = None) -> None:\n",
        "        \"\"\"Download a file from S3 with progress tracking.\"\"\"\n",
        "        try:\n",
        "            kwargs = {\"Bucket\": bucket, \"Key\": key}\n",
        "            if version_id:\n",
        "                kwargs[\"VersionId\"] = version_id\n",
        "\n",
        "            object_size = self.client.head_object(**kwargs)[\"ContentLength\"]\n",
        "            \n",
        "            with tqdm(total=object_size, unit='B', unit_scale=True, desc=str(filename)) as pbar:\n",
        "                self.client.download_file(\n",
        "                    Bucket=bucket,\n",
        "                    Key=key,\n",
        "                    Filename=str(filename),\n",
        "                    Callback=pbar.update\n",
        "                )\n",
        "        except ClientError as e:\n",
        "            logger.error(f\"Error downloading {key}: {e}\")\n",
        "            raise\n",
        "\n",
        "class VideoProcessor:\n",
        "    MOVIE_EXTENSIONS = {'.wmv', '.mpg', '.mov', '.avi', '.mp4', '.MOV', '.MP4'}\n",
        "    \n",
        "    def __init__(self, s3_client: S3Client, bucket: str):\n",
        "        self.s3_client = s3_client\n",
        "        self.bucket = bucket\n",
        "        self.download_dir = Path(\"downloaded_movies\")\n",
        "        self.output_dir = Path(\"concatenated_videos\")\n",
        "        \n",
        "        # Create necessary directories\n",
        "        self.download_dir.mkdir(exist_ok=True)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Find and verify ffmpeg\n",
        "        self.ffmpeg_path = self._find_ffmpeg()\n",
        "        if not self.ffmpeg_path:\n",
        "            raise RuntimeError(\n",
        "                \"ffmpeg not found. Please install ffmpeg:\\n\"\n",
        "                \"1. Download from https://github.com/BtbN/FFmpeg-Builds/releases\\n\"\n",
        "                \"2. Extract the zip file\\n\"\n",
        "                \"3. Add the bin folder to your system PATH or place ffmpeg.exe in your working directory\"\n",
        "            )\n",
        "\n",
        "    def _find_ffmpeg(self) -> Optional[str]:\n",
        "        \"\"\"Find ffmpeg executable in various locations.\"\"\"\n",
        "        try:\n",
        "            # Check if ffmpeg is in PATH\n",
        "            result = subprocess.run(['ffmpeg', '-version'], \n",
        "                                 capture_output=True, \n",
        "                                 check=False)\n",
        "            if result.returncode == 0:\n",
        "                return 'ffmpeg'\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "        # Check common Windows locations\n",
        "        possible_paths = [\n",
        "            Path.cwd() / \"ffmpeg.exe\",  # Current directory\n",
        "            Path.cwd() / \"bin\" / \"ffmpeg.exe\",  # bin subdirectory\n",
        "            Path(os.getenv('PROGRAMFILES', '')) / \"ffmpeg\" / \"bin\" / \"ffmpeg.exe\",\n",
        "            Path(os.getenv('PROGRAMFILES(X86)', '')) / \"ffmpeg\" / \"bin\" / \"ffmpeg.exe\",\n",
        "        ]\n",
        "\n",
        "        # Add conda environment path if running in conda\n",
        "        conda_prefix = os.getenv('CONDA_PREFIX')\n",
        "        if conda_prefix:\n",
        "            possible_paths.append(Path(conda_prefix) / \"Library\" / \"bin\" / \"ffmpeg.exe\")\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                logger.info(f\"Found ffmpeg at: {path}\")\n",
        "                return str(path)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def verify_video_file(self, file_path: Path) -> bool:\n",
        "        \"\"\"Verify that a video file exists and has non-zero size.\"\"\"\n",
        "        try:\n",
        "            if not file_path.exists():\n",
        "                logger.error(f\"Video file does not exist: {file_path}\")\n",
        "                return False\n",
        "            \n",
        "            size = file_path.stat().st_size\n",
        "            if size == 0:\n",
        "                logger.error(f\"Video file is empty: {file_path}\")\n",
        "                return False\n",
        "                \n",
        "            # Try to read video metadata\n",
        "            cmd = [\n",
        "                self.ffmpeg_path,\n",
        "                '-v', 'error',\n",
        "                '-i', str(file_path),\n",
        "                '-f', 'null',\n",
        "                '-'\n",
        "            ]\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "            if result.returncode != 0:\n",
        "                logger.error(f\"Invalid video file {file_path}: {result.stderr}\")\n",
        "                return False\n",
        "                \n",
        "            logger.info(f\"Verified valid video file: {file_path} (size: {size/1024/1024:.2f} MB)\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error verifying video file {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def concatenate_videos(self, video_paths: List[Path], output_path: Path, verify_videos: bool) -> bool:\n",
        "        \"\"\"Concatenate multiple videos using ffmpeg.\"\"\"\n",
        "        list_file = None  # Define outside try block so it's available in finally\n",
        "        try:\n",
        "            if verify_videos:\n",
        "                # Verify all input files exist and are valid\n",
        "                logger.info(f\"Verifying {len(video_paths)} input videos...\")\n",
        "                valid_videos = []\n",
        "                for path in video_paths:\n",
        "                    if self.verify_video_file(path):\n",
        "                        valid_videos.append(path)\n",
        "                    else:\n",
        "                        logger.error(f\"Skipping invalid video: {path}\")\n",
        "                        \n",
        "                if not valid_videos:\n",
        "                    raise ValueError(\"No valid videos found to concatenate\")\n",
        "                \n",
        "                if len(valid_videos) != len(video_paths):\n",
        "                    logger.warning(f\"Only {len(valid_videos)} out of {len(video_paths)} videos are valid\")\n",
        "            else:\n",
        "                valid_videos = video_paths\n",
        "                \n",
        "            total_input_size = sum(path.stat().st_size for path in valid_videos)\n",
        "            logger.info(f\"Total input size: {total_input_size/1024/1024:.2f} MB\")\n",
        "            \n",
        "            # Create a temporary file list for ffmpeg\n",
        "            list_file = self.download_dir / \"file_list.txt\"\n",
        "            with open(list_file, 'w', encoding='utf-8') as f:\n",
        "                for path in valid_videos:\n",
        "                    f.write(f\"file '{path.absolute()}'\\n\")\n",
        "            \n",
        "            logger.info(f\"Created concat list file at {list_file}\")\n",
        "            \n",
        "            # Build ffmpeg command with more detailed error reporting\n",
        "            cmd = [\n",
        "                self.ffmpeg_path,\n",
        "                '-v', 'error',  # Only show errors\n",
        "                '-f', 'concat',\n",
        "                '-safe', '0',\n",
        "                '-i', str(list_file),\n",
        "                '-c', 'copy',\n",
        "                '-y',  # Overwrite output if exists\n",
        "                str(output_path)\n",
        "            ]\n",
        "            \n",
        "            logger.info(f\"Running ffmpeg command: {' '.join(cmd)}\")\n",
        "            \n",
        "            # Run the ffmpeg command\n",
        "            start_time = time.time()\n",
        "            result = subprocess.run(\n",
        "                cmd,\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            \n",
        "            if result.returncode != 0:\n",
        "                logger.error(f\"FFmpeg concatenation failed: {result.stderr}\")\n",
        "                return False\n",
        "            \n",
        "            # Verify the output file\n",
        "            if not self.verify_video_file(output_path):\n",
        "                logger.error(\"Output video verification failed\")\n",
        "                return False\n",
        "            \n",
        "            output_size = output_path.stat().st_size\n",
        "            if output_size < total_input_size * 0.9:  # Allow for some variation due to metadata\n",
        "                logger.error(f\"Output file suspiciously small: {output_size/1024/1024:.2f} MB vs expected {total_input_size/1024/1024:.2f} MB\")\n",
        "                return False\n",
        "            \n",
        "            duration = time.time() - start_time\n",
        "            logger.info(f\"Concatenation completed in {duration:.1f} seconds\")\n",
        "            logger.info(f\"Output file size: {output_size/1024/1024:.2f} MB\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during video concatenation: {str(e)}\")\n",
        "            return False\n",
        "        finally:\n",
        "            # Clean up the temporary file list\n",
        "            if list_file and list_file.exists():\n",
        "                try:\n",
        "                    list_file.unlink()\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error cleaning up list file: {str(e)}\")\n",
        "\n",
        "    def _process_single_drop(self, drop_data: pd.DataFrame, delete_originals: bool, test_mode: bool, verify_videos: bool) -> None:\n",
        "        \"\"\"Process a single drop's worth of videos.\"\"\"\n",
        "        downloaded_files = []\n",
        "        output_path = None\n",
        "        \n",
        "        try:\n",
        "            # Download and process files\n",
        "            downloaded_files = self._download_videos(drop_data['Key'])\n",
        "            \n",
        "            # Sort files by name to ensure correct order\n",
        "            downloaded_files.sort()\n",
        "            logger.info(f\"Processing files in order: {[f.name for f in downloaded_files]}\")\n",
        "            \n",
        "            if verify_videos:\n",
        "                # Verify files immediately after download\n",
        "                valid_files = []\n",
        "                for file_path in downloaded_files:\n",
        "                    if self.verify_video_file(file_path):\n",
        "                        valid_files.append(file_path)\n",
        "                    else:\n",
        "                        logger.error(f\"Downloaded file is corrupted: {file_path}\")\n",
        "                \n",
        "                if not valid_files:\n",
        "                    raise RuntimeError(\"No valid video files available for processing\")\n",
        "            \n",
        "            else:\n",
        "                valid_files = downloaded_files            \n",
        "            \n",
        "            output_path = self._concatenate_drop_videos(valid_files, drop_data['DropID'].iloc[0], verify_videos)\n",
        "            \n",
        "            if not test_mode:\n",
        "                self._upload_and_cleanup(output_path, drop_data, delete_originals)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing drop: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            # Ensure cleanup happens even if there's an error\n",
        "            self._cleanup_files(downloaded_files, output_path)\n",
        "            \n",
        "    def _verify_ffmpeg(self) -> None:\n",
        "        \"\"\"Verify that ffmpeg is installed and accessible.\"\"\"\n",
        "        try:\n",
        "            subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)\n",
        "            logger.info(\"ffmpeg installation verified successfully\")\n",
        "        except subprocess.SubprocessError:\n",
        "            logger.error(\"ffmpeg is not installed or not accessible in system PATH\")\n",
        "            raise RuntimeError(\"ffmpeg is required but not found. Please install ffmpeg first.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error verifying ffmpeg installation: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    \n",
        "\n",
        "    def process_gopro_videos(\n",
        "        self,\n",
        "        filtered_df: pd.DataFrame,\n",
        "        delete_originals: bool = False,\n",
        "        test_mode: bool = False,\n",
        "        gopro_prefix: str = \"GX\",\n",
        "        verify_videos: bool = False\n",
        "    ) -> None:\n",
        "        \"\"\"Process GoPro videos by DropID.\"\"\"\n",
        "        for drop_id in filtered_df['DropID'].unique():\n",
        "            drop_data = filtered_df[filtered_df['DropID'] == drop_id]\n",
        "            \n",
        "            if not all(str(name).startswith(gopro_prefix) for name in drop_data['fileName']):\n",
        "                logger.warning(f\"Skipping DropID {drop_id}: Not all videos start with {gopro_prefix}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                self._process_single_drop(drop_data, delete_originals, test_mode, verify_videos)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing DropID {drop_id}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    def get_movies_df(self, prefix: str = \"\") -> pd.DataFrame:\n",
        "        \"\"\"Get DataFrame of movie files in S3 bucket.\"\"\"\n",
        "        movie_keys = [\n",
        "            obj[\"Key\"] for obj in self.s3_client.list_objects(\n",
        "                self.bucket,\n",
        "                prefix=prefix,\n",
        "                suffix=tuple(self.MOVIE_EXTENSIONS)\n",
        "            )\n",
        "        ]\n",
        "        return pd.DataFrame(movie_keys, columns=[\"Key\"])\n",
        "\n",
        "    \n",
        "\n",
        "    def _download_videos(self, keys: pd.Series) -> List[Path]:\n",
        "        \"\"\"Download all videos for a drop.\"\"\"\n",
        "        downloaded_files = []\n",
        "        for key in keys:\n",
        "            local_path = self.download_dir / Path(key).name\n",
        "            self.s3_client.download_file(self.bucket, key, local_path)\n",
        "            downloaded_files.append(local_path)\n",
        "        return downloaded_files\n",
        "\n",
        "    def _concatenate_drop_videos(self, video_paths: List[Path], drop_id: str, verify_videos: bool) -> Path:\n",
        "        \"\"\"Concatenate videos for a single drop.\"\"\"\n",
        "        output_path = self.output_dir / f\"{drop_id}.mp4\"\n",
        "        if not self.concatenate_videos(video_paths, output_path, verify_videos):\n",
        "            raise RuntimeError(\"Video concatenation failed\")\n",
        "        return output_path\n",
        "\n",
        "    def _upload_and_cleanup(self, output_path: Path, drop_data: pd.DataFrame, delete_originals: bool) -> None:\n",
        "        \"\"\"Upload concatenated video and cleanup originals if requested.\"\"\"\n",
        "        new_key = f\"{drop_data['SurveyID'].iloc[0]}/{drop_data['DropID'].iloc[0]}/{drop_data['DropID'].iloc[0]}.mp4\"\n",
        "    \n",
        "        # Get file size for progress bar\n",
        "        file_size = output_path.stat().st_size\n",
        "        \n",
        "        # Create a progress bar callback\n",
        "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=f\"Uploading {output_path.name}\") as pbar:\n",
        "            def callback(bytes_transferred):\n",
        "                pbar.update(bytes_transferred - pbar.n)  # Update with the difference\n",
        "        \n",
        "        try:\n",
        "            # Upload concatenated file with progress tracking\n",
        "            self.s3_client.client.upload_file(\n",
        "                str(output_path),\n",
        "                self.bucket,\n",
        "                new_key,\n",
        "                Callback=callback\n",
        "            )\n",
        "            logger.info(f\"Successfully uploaded concatenated video to {new_key}\")\n",
        "            \n",
        "            # Delete original files if requested\n",
        "            if delete_originals:\n",
        "                for key in drop_data['Key']:\n",
        "                    self.s3_client.client.delete_object(Bucket=self.bucket, Key=key)\n",
        "                    logger.info(f\"Deleted original file {key}\")\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during upload of {new_key}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    @staticmethod\n",
        "    def _cleanup_files(downloaded_files: List[Path], output_path: Optional[Path]) -> None:\n",
        "        \"\"\"Clean up local files.\"\"\"\n",
        "        for file_path in downloaded_files:\n",
        "            if file_path.exists():\n",
        "                file_path.unlink()\n",
        "        \n",
        "        if output_path and output_path.exists():\n",
        "            output_path.unlink()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O7I2szT73gG"
      },
      "source": [
        "# Connect to s3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0pRSMF3v1bpd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-10 22:52:07,913 - __main__ - INFO - AWS credentials not found in environment variables. Please enter them manually.\n",
            "2025-01-10 22:52:32,651 - __main__ - INFO - Successfully authenticated with AWS\n"
          ]
        }
      ],
      "source": [
        "# Initialize the S3 client\n",
        "s3_client = S3Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbJb13_D1AhN"
      },
      "source": [
        "# Get info from go pro movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PIqe28PQ072M"
      },
      "outputs": [],
      "source": [
        "# Create video processor\n",
        "processor = VideoProcessor(s3_client, bucket=\"marine-buv\")\n",
        "\n",
        "# Get all movies available\n",
        "movies_df = processor.get_movies_df(prefix=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "gopro_ext = \"GH\"\n",
        "\n",
        "# Create the SurveyID, DropID, and fileName columns from the Key\n",
        "movies_df = movies_df.assign(\n",
        "    SurveyID=movies_df['Key'].str.split('/', expand=True)[0],\n",
        "    DropID=movies_df['Key'].str.split('/', expand=True)[1],\n",
        "    fileName=movies_df['Key'].str.split('/', expand=True)[2]\n",
        ")\n",
        "\n",
        "go_pro_movies_df = movies_df[movies_df.fileName.str.startswith(gopro_ext)]\n",
        "\n",
        "# Group by 'DropID' and count unique 'fileName' values for each group\n",
        "grouped_counts = go_pro_movies_df.groupby('DropID')['fileName'].nunique()\n",
        "\n",
        "# Filter for 'DropID's with more than one unique 'fileName'\n",
        "filtered_dropids = grouped_counts[grouped_counts > 1].index\n",
        "\n",
        "# Filter the original DataFrame to retain only rows with the filtered 'DropID's\n",
        "filtered_df = go_pro_movies_df[go_pro_movies_df['DropID'].isin(filtered_dropids)]\n",
        "\n",
        "# print(filtered_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    Key          SurveyID  \\\n",
            "3366  TUH_20240307_BUV/TUH_20240307_BUV_TUH_105_02/G...  TUH_20240307_BUV   \n",
            "3367  TUH_20240307_BUV/TUH_20240307_BUV_TUH_105_02/G...  TUH_20240307_BUV   \n",
            "3368  TUH_20240307_BUV/TUH_20240307_BUV_TUH_105_02/G...  TUH_20240307_BUV   \n",
            "\n",
            "                           DropID      fileName  \n",
            "3366  TUH_20240307_BUV_TUH_105_02  GH010136.MP4  \n",
            "3367  TUH_20240307_BUV_TUH_105_02  GH020136.MP4  \n",
            "3368  TUH_20240307_BUV_TUH_105_02  GH030136.MP4  \n"
          ]
        }
      ],
      "source": [
        "print(filtered_df.tail(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "downloaded_movies\\GH010136.MP4: 100%|██████████| 4.00G/4.00G [01:50<00:00, 36.2MB/s]\n",
            "downloaded_movies\\GH020136.MP4: 100%|██████████| 4.00G/4.00G [01:49<00:00, 36.4MB/s] \n",
            "downloaded_movies\\GH030136.MP4: 100%|██████████| 861M/861M [00:23<00:00, 35.9MB/s] \n",
            "2025-01-10 22:57:33,552 - __main__ - INFO - Processing files in order: ['GH010136.MP4', 'GH020136.MP4', 'GH030136.MP4']\n",
            "2025-01-10 22:57:33,553 - __main__ - INFO - Total input size: 8454.56 MB\n",
            "2025-01-10 22:57:33,554 - __main__ - INFO - Created concat list file at downloaded_movies\\file_list.txt\n",
            "2025-01-10 22:57:33,555 - __main__ - INFO - Running ffmpeg command: ffmpeg -v error -f concat -safe 0 -i downloaded_movies\\file_list.txt -c copy -y concatenated_videos\\TUH_20240307_BUV_TUH_105_02.mp4\n",
            "2025-01-10 23:02:09,140 - __main__ - INFO - Verified valid video file: concatenated_videos\\TUH_20240307_BUV_TUH_105_02.mp4 (size: 8437.95 MB)\n",
            "2025-01-10 23:02:09,146 - __main__ - INFO - Concatenation completed in 275.6 seconds\n",
            "2025-01-10 23:02:09,146 - __main__ - INFO - Output file size: 8437.95 MB\n",
            "Uploading TUH_20240307_BUV_TUH_105_02.mp4:   0%|          | 0.00/8.85G [00:00<?, ?B/s]\n",
            "2025-01-10 23:14:22,939 - __main__ - INFO - Successfully uploaded concatenated video to TUH_20240307_BUV/TUH_20240307_BUV_TUH_105_02/TUH_20240307_BUV_TUH_105_02.mp4\n"
          ]
        }
      ],
      "source": [
        "processor.process_gopro_videos(\n",
        "    filtered_df=filtered_df.tail(3),\n",
        "    delete_originals=False,\n",
        "    test_mode=False,\n",
        "    gopro_prefix= gopro_ext,\n",
        "    verify_videos=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPWNn5s5FtwbB065Ta3vhnh",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
