{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script looks for GoPro video files in AWS and concatenates them using the \"dropID\" part of the Key as its filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_qqwckw8DDE"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from typing import List, Iterator, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from botocore.exceptions import ClientError\n",
        "import getpass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import tempfile\n",
        "\n",
        "\n",
        "# Configure logging with a more detailed format\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class AWSCredentials:\n",
        "    access_key_id: str\n",
        "    secret_access_key: str\n",
        "    \n",
        "    @classmethod\n",
        "    def from_user_input(cls) -> 'AWSCredentials':\n",
        "        \"\"\"Securely prompt user for AWS credentials.\"\"\"\n",
        "        access_key = getpass.getpass(\"Enter AWS Access Key ID: \")\n",
        "        secret_key = getpass.getpass(\"Enter AWS Secret Access Key: \")\n",
        "        return cls(access_key, secret_key)\n",
        "\n",
        "class S3Client:\n",
        "    def __init__(self, credentials: Optional[AWSCredentials] = None, max_connections: int = 10):\n",
        "        self.max_connections = max_connections\n",
        "        self.client = self._initialize_client(credentials)\n",
        "\n",
        "    def _initialize_client(self, credentials: Optional[AWSCredentials]) -> boto3.client:\n",
        "        \"\"\"Initialize S3 client with credentials from env vars, provided credentials, or user input.\"\"\"\n",
        "        if credentials is None:\n",
        "            # Try environment variables first\n",
        "            access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "            secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "            \n",
        "            if not access_key or not secret_key:\n",
        "                logger.info(\"AWS credentials not found in environment variables. Please enter them manually.\")\n",
        "                credentials = AWSCredentials.from_user_input()\n",
        "            else:\n",
        "                credentials = AWSCredentials(access_key, secret_key)\n",
        "\n",
        "        try:\n",
        "            # Configure for better performance and reliability\n",
        "            config = boto3.session.Config(\n",
        "                max_pool_connections=self.max_connections,\n",
        "                retries={\n",
        "                    'max_attempts': 5,\n",
        "                    'mode': 'adaptive'\n",
        "                },\n",
        "                connect_timeout=120,\n",
        "                read_timeout=300,\n",
        "                # Add TCP keepalive\n",
        "                tcp_keepalive=True\n",
        "            )\n",
        "            \n",
        "            client = boto3.client(\n",
        "                \"s3\",\n",
        "                aws_access_key_id=credentials.access_key_id,\n",
        "                aws_secret_access_key=credentials.secret_access_key,\n",
        "                config=config\n",
        "            )\n",
        "            # Test the credentials by making a simple API call\n",
        "            client.list_buckets()\n",
        "            logger.info(\"Successfully authenticated with AWS\")\n",
        "            return client\n",
        "        except ClientError as e:\n",
        "            logger.error(\"Failed to authenticate with AWS\")\n",
        "            if \"InvalidAccessKeyId\" in str(e) or \"SignatureDoesNotMatch\" in str(e):\n",
        "                logger.error(\"Invalid credentials provided. Please try again.\")\n",
        "                credentials = AWSCredentials.from_user_input()\n",
        "                return self._initialize_client(credentials)\n",
        "            raise\n",
        "\n",
        "    def list_objects(self, bucket: str, prefix: str = \"\", suffix: str = \"\") -> Iterator[dict]:\n",
        "        \"\"\"List objects in an S3 bucket with optional prefix and suffix filtering.\"\"\"\n",
        "        paginator = self.client.get_paginator(\"list_objects_v2\")\n",
        "        \n",
        "        for prefix_item in [prefix] if isinstance(prefix, str) else prefix:\n",
        "            try:\n",
        "                for page in paginator.paginate(Bucket=bucket, Prefix=prefix_item):\n",
        "                    if \"Contents\" not in page:\n",
        "                        continue\n",
        "                    \n",
        "                    for obj in page[\"Contents\"]:\n",
        "                        if obj[\"Key\"].endswith(suffix):\n",
        "                            yield obj\n",
        "            except ClientError as e:\n",
        "                logger.error(f\"Error listing objects: {e}\")\n",
        "                raise\n",
        "\n",
        "    def download_file_with_retry(self, bucket: str, key: str, filename: Path, max_retries: int = 3) -> bool:\n",
        "        \"\"\"Download a file from S3 with retry logic and integrity checking.\"\"\"\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                logger.info(f\"Downloading {key} (attempt {attempt + 1}/{max_retries})\")\n",
        "                \n",
        "                # Get object metadata first to check expected size\n",
        "                response = self.client.head_object(Bucket=bucket, Key=key)\n",
        "                expected_size = response['ContentLength']\n",
        "                \n",
        "                # Download the file\n",
        "                self.client.download_file(\n",
        "                    Bucket=bucket,\n",
        "                    Key=key,\n",
        "                    Filename=str(filename)\n",
        "                )\n",
        "                \n",
        "                # Verify the download\n",
        "                if filename.exists():\n",
        "                    actual_size = filename.stat().st_size\n",
        "                    if actual_size == expected_size:\n",
        "                        logger.info(f\"✅ Successfully downloaded {key} ({actual_size/1024/1024:.2f} MB)\")\n",
        "                        return True\n",
        "                    else:\n",
        "                        logger.warning(f\"❌ Size mismatch for {key}: expected {expected_size}, got {actual_size}\")\n",
        "                        filename.unlink(missing_ok=True)\n",
        "                else:\n",
        "                    logger.warning(f\"❌ Downloaded file {filename} does not exist\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                logger.error(f\"❌ Download attempt {attempt + 1} failed for {key}: {str(e)}\")\n",
        "                filename.unlink(missing_ok=True)\n",
        "                \n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = 2 ** attempt  # Exponential backoff\n",
        "                    logger.info(f\"Waiting {wait_time} seconds before retry...\")\n",
        "                    time.sleep(wait_time)\n",
        "        \n",
        "        logger.error(f\"❌ Failed to download {key} after {max_retries} attempts\")\n",
        "        return False\n",
        "\n",
        "class VideoProcessor:\n",
        "    MOVIE_EXTENSIONS = {'.wmv', '.mpg', '.mov', '.avi', '.mp4', '.MOV', '.MP4'}\n",
        "    \n",
        "    def __init__(self, s3_client: S3Client, bucket: str, max_workers: int = 2):\n",
        "        self.s3_client = s3_client\n",
        "        self.bucket = bucket\n",
        "        self.max_workers = max_workers  # Reduced to prevent connection pool exhaustion\n",
        "        self.download_dir = Path(\"downloaded_movies\")\n",
        "        self.output_dir = Path(\"concatenated_videos\")\n",
        "        \n",
        "        # Create necessary directories\n",
        "        self.download_dir.mkdir(exist_ok=True)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Find and verify ffmpeg\n",
        "        self.ffmpeg_path = self._find_ffmpeg()\n",
        "        if not self.ffmpeg_path:\n",
        "            raise RuntimeError(\n",
        "                \"ffmpeg not found. Please install ffmpeg:\\n\"\n",
        "                \"1. Download from https://github.com/BtbN/FFmpeg-Builds/releases\\n\"\n",
        "                \"2. Extract the zip file\\n\"\n",
        "                \"3. Add the bin folder to your system PATH or place ffmpeg.exe in your working directory\"\n",
        "            )\n",
        "\n",
        "    def _find_ffmpeg(self) -> Optional[str]:\n",
        "        \"\"\"Find ffmpeg executable in various locations.\"\"\"\n",
        "        try:\n",
        "            # Check if ffmpeg is in PATH\n",
        "            result = subprocess.run(['ffmpeg', '-version'], \n",
        "                                 capture_output=True, \n",
        "                                 check=False)\n",
        "            if result.returncode == 0:\n",
        "                return 'ffmpeg'\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "        # Check common Windows locations\n",
        "        possible_paths = [\n",
        "            Path.cwd() / \"ffmpeg.exe\",  # Current directory\n",
        "            Path.cwd() / \"bin\" / \"ffmpeg.exe\",  # bin subdirectory\n",
        "            Path(os.getenv('PROGRAMFILES', '')) / \"ffmpeg\" / \"bin\" / \"ffmpeg.exe\",\n",
        "            Path(os.getenv('PROGRAMFILES(X86)', '')) / \"ffmpeg\" / \"bin\" / \"ffmpeg.exe\",\n",
        "        ]\n",
        "\n",
        "        # Add conda environment path if running in conda\n",
        "        conda_prefix = os.getenv('CONDA_PREFIX')\n",
        "        if conda_prefix:\n",
        "            possible_paths.append(Path(conda_prefix) / \"Library\" / \"bin\" / \"ffmpeg.exe\")\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                logger.info(f\"Found ffmpeg at: {path}\")\n",
        "                return str(path)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def verify_video_file(self, file_path: Path) -> bool:\n",
        "        \"\"\"Verify that a video file exists and has non-zero size.\"\"\"\n",
        "        try:\n",
        "            if not file_path.exists():\n",
        "                logger.error(f\"Video file does not exist: {file_path}\")\n",
        "                return False\n",
        "            \n",
        "            size = file_path.stat().st_size\n",
        "            if size == 0:\n",
        "                logger.error(f\"Video file is empty: {file_path}\")\n",
        "                return False\n",
        "                \n",
        "            logger.info(f\"Verified video file: {file_path} (size: {size/1024/1024:.2f} MB)\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error verifying video file {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def verify_video_file_deep(self, file_path: Path) -> bool:\n",
        "        \"\"\"Deep verification using ffmpeg probe.\"\"\"\n",
        "        try:\n",
        "            # Try to read video metadata using ffprobe (more lightweight than ffmpeg)\n",
        "            cmd = [\n",
        "                'ffprobe',\n",
        "                '-v', 'quiet',\n",
        "                '-print_format', 'json',\n",
        "                '-show_format',\n",
        "                '-show_streams',\n",
        "                str(file_path)\n",
        "            ]\n",
        "            \n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n",
        "            if result.returncode != 0:\n",
        "                logger.error(f\"ffprobe failed for {file_path}: {result.stderr}\")\n",
        "                return False\n",
        "            \n",
        "            # Check if we got valid JSON output\n",
        "            import json\n",
        "            try:\n",
        "                data = json.loads(result.stdout)\n",
        "                if 'format' in data and 'streams' in data:\n",
        "                    logger.info(f\"✅ Video file verified: {file_path}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    logger.error(f\"Invalid video format for {file_path}\")\n",
        "                    return False\n",
        "            except json.JSONDecodeError:\n",
        "                logger.error(f\"Invalid ffprobe output for {file_path}\")\n",
        "                return False\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            logger.error(f\"ffprobe timeout for {file_path}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error verifying video file {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def _try_repair_video(self, corrupted_path: Path, temp_dir: Path) -> Optional[Path]:\n",
        "        \"\"\"\n",
        "        Attempts to repair a corrupted video file by re-muxing it.\n",
        "\n",
        "        Args:\n",
        "            corrupted_path: The path to the potentially corrupted video.\n",
        "            temp_dir: The temporary directory to use for this repair operation.\n",
        "\n",
        "        Returns:\n",
        "            The path to the repaired video file if successful, otherwise None.\n",
        "        \"\"\"\n",
        "        repaired_path = temp_dir / f\"repaired_{corrupted_path.name}\"\n",
        "        logger.warning(f\"Attempting to repair corrupted video: {corrupted_path}\")\n",
        "        logger.info(f\"Repaired file will be saved to: {repaired_path}\")\n",
        "\n",
        "        # Command to fix video by re-muxing (copying codecs without re-encoding)\n",
        "        # This is fast and often fixes container-level corruption.\n",
        "        cmd = [\n",
        "            self.ffmpeg_path, '-y', '-i', str(corrupted_path),\n",
        "            '-c', 'copy',           # Copy video and audio streams without re-encoding\n",
        "            '-ignore_unknown',      # Ignore any unknown streams\n",
        "            str(repaired_path)\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                cmd, capture_output=True, text=True, check=True, encoding='utf-8', timeout=600\n",
        "            )\n",
        "            logger.info(f\"Successfully re-muxed video: {corrupted_path}\")\n",
        "\n",
        "            # Verify that the new file is valid\n",
        "            if self.verify_video_file_deep(repaired_path):\n",
        "                logger.info(f\"✅ Repair successful. New file is valid: {repaired_path}\")\n",
        "                return repaired_path\n",
        "            else:\n",
        "                logger.error(\"❌ Repair command ran, but the output file is invalid.\")\n",
        "                return None\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            logger.error(f\"❌ Failed to repair video '{corrupted_path}'. FFmpeg returned an error.\")\n",
        "            logger.error(f\"Stderr:\\n{e.stderr}\")\n",
        "            return None\n",
        "        except subprocess.TimeoutExpired:\n",
        "            logger.error(f\"❌ Repair attempt for '{corrupted_path}' timed out.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ An unexpected error occurred during repair: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def concatenate_videos(self, video_paths: List[Path], output_path: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Concatenate multiple videos, attempting to repair any corrupted files first.\n",
        "        If a file cannot be repaired, the entire operation is aborted.\n",
        "        \"\"\"\n",
        "        list_file = None\n",
        "        \n",
        "        # Create a unique temporary directory for this concatenation task\n",
        "        temp_dir = Path(tempfile.mkdtemp(prefix=\"video_repair_\"))\n",
        "        logger.info(f\"Using temporary directory for repairs: {temp_dir}\")\n",
        "\n",
        "        try:\n",
        "            # --- Verification and Repair Stage ---\n",
        "            logger.info(f\"Verifying and preparing {len(video_paths)} input videos...\")\n",
        "            videos_to_concat = []\n",
        "            for path in video_paths:\n",
        "                if self.verify_video_file_deep(path):\n",
        "                    videos_to_concat.append(path)\n",
        "                else:\n",
        "                    logger.warning(f\"Invalid or corrupted video detected: {path}\")\n",
        "                    repaired_path = self._try_repair_video(path, temp_dir)\n",
        "\n",
        "                    if repaired_path:\n",
        "                        videos_to_concat.append(repaired_path)\n",
        "                    else:\n",
        "                        logger.critical(f\"Could not repair '{path}'. Aborting concatenation.\")\n",
        "                        return False # This will trigger the finally block\n",
        "\n",
        "            if not videos_to_concat:\n",
        "                raise ValueError(\"No valid videos available to concatenate.\")\n",
        "            \n",
        "                            \n",
        "            # --- Concatenation Stage ---\n",
        "            total_input_size = sum(path.stat().st_size for path in videos_to_concat)\n",
        "            logger.info(f\"Total input size: {total_input_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "            list_file = self.download_dir / \"file_list.txt\"\n",
        "            with open(list_file, 'w', encoding='utf-8') as f:\n",
        "                for path in videos_to_concat:\n",
        "                    # Use forward slashes for cross-platform compatibility\n",
        "                    f.write(f\"file '{str(path.resolve()).replace(chr(92), '/')}'\\n\")\n",
        "            logger.info(f\"Created concat list file at {list_file}\")\n",
        "\n",
        "            base_cmd = [self.ffmpeg_path, '-y', '-f', 'concat', '-safe', '0', '-i', str(list_file)]\n",
        "            codec_cmd = ['-c', 'copy'] # Use stream copy for speed\n",
        "            cmd = base_cmd + codec_cmd + [str(output_path)]\n",
        "\n",
        "            logger.info(f\"Running command: {' '.join(cmd)}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            subprocess.run(\n",
        "                cmd, capture_output=True, text=True, check=True, encoding='utf-8', timeout=3600\n",
        "            )\n",
        "\n",
        "            end_time = time.time()\n",
        "            logger.info(f\"✅ Success! Concatenation took {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "            if not output_path.exists() or output_path.stat().st_size == 0:\n",
        "                logger.error(\"Output video file is missing or empty after concatenation.\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Output file size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "            return True\n",
        "                \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            logger.error(\"❌ FFmpeg concatenation command failed!\")\n",
        "            logger.error(f\"Stderr:\\n{e.stderr}\")\n",
        "            return False\n",
        "        except subprocess.TimeoutExpired:\n",
        "            logger.error(\"❌ FFmpeg concatenation command timed out!\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Concatenation failed: {e}\")\n",
        "            return False\n",
        "        finally:\n",
        "            # --- Cleanup Stage ---\n",
        "            # Clean up the concat list file\n",
        "            if list_file and list_file.exists():\n",
        "                try:\n",
        "                    list_file.unlink()\n",
        "                except OSError as e:\n",
        "                    logger.error(f\"Error cleaning up list file: {e}\")\n",
        "\n",
        "            # Clean up temporary repaired files and temp directory\n",
        "            if temp_dir and temp_dir.exists():\n",
        "                logger.info(f\"Cleaning up temporary directory: {temp_dir}\")\n",
        "                try:\n",
        "                    # Remove any files in temp_dir first\n",
        "                    for temp_file in temp_dir.iterdir():\n",
        "                        try:\n",
        "                            if temp_file.is_file():\n",
        "                                temp_file.unlink()\n",
        "                            elif temp_file.is_dir():\n",
        "                                temp_file.rmdir()\n",
        "                        except Exception as e:  # Catch all exceptions, not just OSError\n",
        "                            logger.error(f\"Error removing temp file/dir '{temp_file}': {e}\")\n",
        "                    \n",
        "                    # Remove the temp directory itself\n",
        "                    temp_dir.rmdir()\n",
        "                    logger.debug(f\"Cleaned up temporary directory: {temp_dir}\")\n",
        "                except Exception as e:  # Catch all exceptions for directory removal too\n",
        "                    logger.error(f\"Error removing temporary directory '{temp_dir}': {e}\")\n",
        "                    \n",
        "                    # If standard cleanup fails, try more aggressive cleanup\n",
        "                    try:\n",
        "                        import shutil\n",
        "                        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "                        logger.debug(f\"Force-cleaned temporary directory using shutil.rmtree: {temp_dir}\")\n",
        "                    except Exception as fallback_e:\n",
        "                        logger.error(f\"Failed to force-clean temporary directory: {fallback_e}\")\n",
        "                        logger.warning(f\"Temporary directory may need manual cleanup: {temp_dir}\")\n",
        "\n",
        "    def _download_single_video(self, key: str, progress_dict: dict = None) -> Path:\n",
        "        \"\"\"Download a single video file with retry logic.\"\"\"\n",
        "        local_path = self.download_dir / Path(key).name\n",
        "        try:\n",
        "            success = self.s3_client.download_file_with_retry(self.bucket, key, local_path)\n",
        "            if progress_dict is not None:\n",
        "                progress_dict[key] = success\n",
        "            if success:\n",
        "                return local_path\n",
        "            else:\n",
        "                raise RuntimeError(f\"Failed to download {key}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to download {key}: {str(e)}\")\n",
        "            if progress_dict is not None:\n",
        "                progress_dict[key] = False\n",
        "            raise\n",
        "\n",
        "    def _download_videos_sequential(self, keys: pd.Series) -> List[Path]:\n",
        "        \"\"\"Download all videos for a drop sequentially (more reliable for large files).\"\"\"\n",
        "        downloaded_files = []\n",
        "        \n",
        "        for key in keys:\n",
        "            try:\n",
        "                local_path = self._download_single_video(key)\n",
        "                downloaded_files.append(local_path)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Download failed for {key}: {str(e)}\")\n",
        "                # Continue with other files\n",
        "                continue\n",
        "        \n",
        "        # Sort files by name to ensure correct order\n",
        "        downloaded_files.sort()\n",
        "        return downloaded_files\n",
        "\n",
        "    def _download_videos_parallel(self, keys: pd.Series) -> List[Path]:\n",
        "        \"\"\"Download all videos for a drop in parallel.\"\"\"\n",
        "        downloaded_files = []\n",
        "        progress_dict = {}\n",
        "        \n",
        "        # Use ThreadPoolExecutor for parallel downloads\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Submit all download tasks\n",
        "            future_to_key = {\n",
        "                executor.submit(self._download_single_video, key, progress_dict): key \n",
        "                for key in keys\n",
        "            }\n",
        "            \n",
        "            # Process completed downloads\n",
        "            for future in as_completed(future_to_key):\n",
        "                key = future_to_key[future]\n",
        "                try:\n",
        "                    local_path = future.result()\n",
        "                    downloaded_files.append(local_path)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Download failed for {key}: {str(e)}\")\n",
        "        \n",
        "        # Sort files by name to ensure correct order\n",
        "        downloaded_files.sort()\n",
        "        return downloaded_files\n",
        "\n",
        "    def _process_single_drop(self, drop_data: pd.DataFrame, delete_originals: bool, test_mode: bool, verify_videos: bool, sequential_download: bool = True) -> None:\n",
        "        \"\"\"Process a single drop's worth of videos.\"\"\"\n",
        "        downloaded_files = []\n",
        "        output_path = None\n",
        "        \n",
        "        try:\n",
        "            # Download files (sequential is more reliable for large files)\n",
        "            if sequential_download:\n",
        "                downloaded_files = self._download_videos_sequential(drop_data['Key'])\n",
        "            else:\n",
        "                downloaded_files = self._download_videos_parallel(drop_data['Key'])\n",
        "            \n",
        "            if not downloaded_files:\n",
        "                raise RuntimeError(\"No files were successfully downloaded\")\n",
        "            \n",
        "            logger.info(f\"Processing files in order: {[f.name for f in downloaded_files]}\")\n",
        "            \n",
        "            if verify_videos:\n",
        "                # Quick verification of downloaded files\n",
        "                valid_files = []\n",
        "                for file_path in downloaded_files:\n",
        "                    if self.verify_video_file(file_path):\n",
        "                        valid_files.append(file_path)\n",
        "                    else:\n",
        "                        logger.error(f\"Downloaded file is corrupted: {file_path}\")\n",
        "                \n",
        "                if not valid_files:\n",
        "                    raise RuntimeError(\"No valid video files available for processing\")\n",
        "            else:\n",
        "                valid_files = downloaded_files            \n",
        "            \n",
        "            output_path = self._concatenate_drop_videos(valid_files, drop_data['DropID'].iloc[0])\n",
        "            \n",
        "            if not test_mode:\n",
        "                self._upload_and_cleanup(output_path, drop_data, delete_originals)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing drop: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            # Ensure cleanup happens even if there's an error\n",
        "            self._cleanup_files(downloaded_files, output_path)\n",
        "\n",
        "    def process_gopro_videos(\n",
        "        self,\n",
        "        filtered_df: pd.DataFrame,\n",
        "        delete_originals: bool = False,\n",
        "        test_mode: bool = False,\n",
        "        gopro_prefix: str = \"GX\",\n",
        "        verify_videos: bool = False,\n",
        "        parallel_drops: bool = False,\n",
        "        sequential_download: bool = True\n",
        "    ) -> None:\n",
        "        \"\"\"Process GoPro videos by DropID with optional parallel processing.\"\"\"\n",
        "        \n",
        "        drop_ids = filtered_df['DropID'].unique()\n",
        "        valid_drops = []\n",
        "        \n",
        "        # Filter valid drops\n",
        "        for drop_id in drop_ids:\n",
        "            drop_data = filtered_df[filtered_df['DropID'] == drop_id]\n",
        "            if all(str(name).startswith(gopro_prefix) for name in drop_data['fileName']):\n",
        "                valid_drops.append((drop_id, drop_data))\n",
        "            else:\n",
        "                logger.warning(f\"Skipping DropID {drop_id}: Not all videos start with {gopro_prefix}\")\n",
        "        \n",
        "        logger.info(f\"Processing {len(valid_drops)} valid drops\")\n",
        "        \n",
        "        if parallel_drops and len(valid_drops) > 1:\n",
        "            # Process drops in parallel (careful with resource usage)\n",
        "            with ThreadPoolExecutor(max_workers=min(2, len(valid_drops))) as executor:\n",
        "                futures = []\n",
        "                for drop_id, drop_data in valid_drops:\n",
        "                    future = executor.submit(\n",
        "                        self._process_single_drop, \n",
        "                        drop_data, delete_originals, test_mode, verify_videos, sequential_download\n",
        "                    )\n",
        "                    futures.append((drop_id, future))\n",
        "                \n",
        "                for drop_id, future in futures:\n",
        "                    try:\n",
        "                        future.result()\n",
        "                        logger.info(f\"✅ Successfully processed DropID {drop_id}\")\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"❌ Error processing DropID {drop_id}: {str(e)}\")\n",
        "        else:\n",
        "            # Sequential processing\n",
        "            for drop_id, drop_data in valid_drops:\n",
        "                try:\n",
        "                    logger.info(f\"Processing DropID {drop_id}...\")\n",
        "                    self._process_single_drop(drop_data, delete_originals, test_mode, verify_videos, sequential_download)\n",
        "                    logger.info(f\"✅ Successfully processed DropID {drop_id}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"❌ Error processing DropID {drop_id}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    def get_movies_df(self, prefix: str = \"\") -> pd.DataFrame:\n",
        "        \"\"\"Get DataFrame of movie files in S3 bucket with their sizes.    \n",
        "        Args:\n",
        "            prefix: Optional prefix to filter S3 objects            \n",
        "        Returns:\n",
        "            DataFrame with columns 'Key' and 'Size' (in bytes)\n",
        "        \"\"\"\n",
        "        # Get all objects matching the prefix and movie extensions\n",
        "        objects = self.s3_client.list_objects(\n",
        "            self.bucket,\n",
        "            prefix=prefix,\n",
        "            suffix=tuple(self.MOVIE_EXTENSIONS)\n",
        "        )\n",
        "        \n",
        "        # Extract both keys and sizes\n",
        "        movie_data = [\n",
        "            {\n",
        "                'Key': obj['Key'],\n",
        "                'Size': obj['Size']  # Size in bytes\n",
        "            }\n",
        "            for obj in objects\n",
        "        ]\n",
        "        \n",
        "        return pd.DataFrame(movie_data)\n",
        "\n",
        "    def _concatenate_drop_videos(self, video_paths: List[Path], drop_id: str) -> Path:\n",
        "        \"\"\"Concatenate videos for a single drop.\"\"\"\n",
        "        output_path = self.output_dir / f\"{drop_id}.mp4\"\n",
        "        if not self.concatenate_videos(video_paths, output_path):\n",
        "            raise RuntimeError(\"Video concatenation failed\")\n",
        "        return output_path\n",
        "\n",
        "    def _upload_and_cleanup(self, output_path: Path, drop_data: pd.DataFrame, delete_originals: bool) -> None:\n",
        "        \"\"\"Upload concatenated video and cleanup originals if requested.\"\"\"\n",
        "        new_key = f\"{drop_data['SurveyID'].iloc[0]}/{drop_data['DropID'].iloc[0]}/{drop_data['DropID'].iloc[0]}.mp4\"\n",
        "        \n",
        "        try:            \n",
        "            self.s3_client.client.upload_file(\n",
        "                str(output_path),\n",
        "                self.bucket,\n",
        "                new_key\n",
        "            )\n",
        "                    \n",
        "            logger.info(f\"Successfully uploaded concatenated video to {new_key}\")\n",
        "            \n",
        "            # Delete original files if requested\n",
        "            if delete_originals:\n",
        "                for key in drop_data['Key']:\n",
        "                    self.s3_client.client.delete_object(Bucket=self.bucket, Key=key)\n",
        "                    logger.info(f\"Deleted original file {key}\")\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during upload of {new_key}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    @staticmethod\n",
        "    def _cleanup_files(downloaded_files: List[Path], output_path: Optional[Path]) -> None:\n",
        "        \"\"\"Clean up local files.\"\"\"\n",
        "        for file_path in downloaded_files:\n",
        "            if file_path.exists():\n",
        "                try:\n",
        "                    file_path.unlink()\n",
        "                    logger.info(f\"Cleaned up {file_path}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error deleting {file_path}: {str(e)}\")\n",
        "        \n",
        "        if output_path and output_path.exists():\n",
        "            try:\n",
        "                output_path.unlink()\n",
        "                logger.info(f\"Cleaned up {output_path}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error deleting {output_path}: {str(e)}\")\n",
        "\n",
        "\n",
        "def get_filtered_movies_df(movies_df: pd.DataFrame, gopro_ext: str = \"BNP\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filter movies DataFrame to remove DropID groups where a file has the same name as its DropID.\n",
        "    \n",
        "    Args:\n",
        "        movies_df: DataFrame with Key column containing file paths\n",
        "        gopro_ext: Prefix used to identify GoPro files\n",
        "    \n",
        "    Returns:\n",
        "        Filtered DataFrame containing only valid GoPro groups\n",
        "    \"\"\"\n",
        "    # Create the SurveyID, DropID, and fileName columns from the Key\n",
        "    df = movies_df.assign(\n",
        "        SurveyID=movies_df['Key'].str.split('/', expand=True)[0],\n",
        "        DropID=movies_df['Key'].str.split('/', expand=True)[1],\n",
        "        fileName=movies_df['Key'].str.split('/', expand=True)[2]\n",
        "    )\n",
        "    \n",
        "    # Filter for GoPro movies\n",
        "    go_pro_movies_df = df[df.fileName.str.startswith(gopro_ext)].copy()\n",
        "    \n",
        "    # Remove .mp4 extension from fileName for comparison\n",
        "    df['fileNameNoExt'] = df['fileName'].str.replace('.mp4', '')\n",
        "    \n",
        "    # Find DropIDs where any fileName (without extension) matches the DropID\n",
        "    matching_dropids = df[\n",
        "        df.apply(\n",
        "            lambda row: row['fileNameNoExt'] == row['DropID'], \n",
        "            axis=1\n",
        "        )\n",
        "    ]['DropID'].unique()\n",
        "    \n",
        "    # Remove groups where DropID matches any fileName\n",
        "    df_no_matching = go_pro_movies_df[~go_pro_movies_df['DropID'].isin(matching_dropids)]\n",
        "    \n",
        "    # Group by 'DropID' and count unique 'fileName' values for each group\n",
        "    grouped_counts = df_no_matching.groupby('DropID')['fileName'].nunique()\n",
        "    \n",
        "    # Filter for 'DropID's with more than one unique 'fileName'\n",
        "    filtered_dropids = grouped_counts[grouped_counts > 1].index\n",
        "    \n",
        "    # Filter the DataFrame to retain only rows with the filtered 'DropID's\n",
        "    filtered_df = df_no_matching[df_no_matching['DropID'].isin(filtered_dropids)]\n",
        "\n",
        "    \n",
        "    return filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbJb13_D1AhN"
      },
      "source": [
        "# Get info from go pro movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIqe28PQ072M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 10:48:57,565 - __main__ - INFO - Successfully authenticated with AWS\n",
            "2025-06-28 10:48:58,589 - __main__ - INFO - Processing 1 valid drops\n",
            "2025-06-28 10:48:58,590 - __main__ - INFO - Processing DropID TON_20211026_BUV_TON_067_01...\n",
            "2025-06-28 10:48:58,591 - __main__ - INFO - Downloading TON_20211026_BUV/TON_20211026_BUV_TON_067_01/GH010978.MP4 (attempt 1/3)\n"
          ]
        }
      ],
      "source": [
        "# Initialize with better connection settings\n",
        "s3_client = S3Client(max_connections=15)\n",
        "\n",
        "# Create video processor with conservative settings\n",
        "processor = VideoProcessor(s3_client, bucket=\"marine-buv\", max_workers=10)\n",
        "\n",
        "# Get movies\n",
        "movies_df = processor.get_movies_df(prefix=\"TON\")\n",
        "gopro_ext = \"G\"\n",
        "filtered_df = get_filtered_movies_df(movies_df=movies_df, gopro_ext=gopro_ext)\n",
        "filtered_df\n",
        "\n",
        "# Process with sequential downloads (more reliable)\n",
        "processor.process_gopro_videos(\n",
        "    filtered_df=filtered_df,\n",
        "    delete_originals=False,\n",
        "    test_mode=False,\n",
        "    gopro_prefix=gopro_ext,\n",
        "    verify_videos=True,  # Enable verification to catch corrupted files\n",
        "    parallel_drops=True,  # Process one drop at a time\n",
        "    sequential_download=True  # Download files one at a time (most reliable)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_df.to_csv(\"filtered_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_already_concatenated_movies_df(movies_df: pd.DataFrame, size_tolerance: float = 0.05) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Find individual video files that should be removed because a concatenated file already exists.\n",
        "    \n",
        "    This function groups files by DropID, orders by size (largest first), and checks if the largest file\n",
        "    has approximately the same size as the sum of all other files. If so, it assumes the largest file\n",
        "    is the concatenated version and returns the smaller files for removal.\n",
        "    \n",
        "    Args:\n",
        "        movies_df: DataFrame with Key column containing file paths and Size column\n",
        "        size_tolerance: Tolerance for size comparison (default 0.05 = 5%)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame containing only individual files that should be removed (where concatenated version exists)\n",
        "    \"\"\"\n",
        "    # Create the SurveyID, DropID, and fileName columns from the Key\n",
        "    df = movies_df.assign(\n",
        "        SurveyID=movies_df['Key'].str.split('/', expand=True)[0],\n",
        "        DropID=movies_df['Key'].str.split('/', expand=True)[1],\n",
        "        fileName=movies_df['Key'].str.split('/', expand=True)[2]\n",
        "    )\n",
        "    \n",
        "    # Only consider DropIDs with multiple files\n",
        "    dropid_counts = df.groupby('DropID').size()\n",
        "    multi_file_dropids = dropid_counts[dropid_counts > 1].index\n",
        "    \n",
        "    print(f\"Found {len(multi_file_dropids)} DropIDs with multiple files\")\n",
        "    \n",
        "    files_to_remove = []\n",
        "    concatenated_found = 0\n",
        "    \n",
        "    for drop_id in multi_file_dropids:\n",
        "        drop_files = df[df['DropID'] == drop_id].copy()\n",
        "        \n",
        "        # Sort by size (largest first)\n",
        "        drop_files = drop_files.sort_values('Size', ascending=False)\n",
        "        \n",
        "        if len(drop_files) < 2:\n",
        "            continue\n",
        "            \n",
        "        largest_file = drop_files.iloc[0]\n",
        "        other_files = drop_files.iloc[1:]\n",
        "        \n",
        "        largest_size = largest_file['Size']\n",
        "        sum_others_size = other_files['Size'].sum()\n",
        "        \n",
        "        # Check if largest file size is approximately equal to sum of others\n",
        "        # Allow for some tolerance due to encoding differences, metadata, etc.\n",
        "        size_ratio = abs(largest_size - sum_others_size) / max(largest_size, sum_others_size)\n",
        "        \n",
        "        if size_ratio <= size_tolerance:\n",
        "            # Largest file is likely concatenated version of the others\n",
        "            files_to_remove.extend(other_files.to_dict('records'))\n",
        "            concatenated_found += 1\n",
        "            \n",
        "            print(f\"DropID {drop_id}: Concatenated file ({largest_size/1024/1024:.1f}MB) ≈ Sum of {len(other_files)} files ({sum_others_size/1024/1024:.1f}MB)\")\n",
        "        else:\n",
        "            print(f\"DropID {drop_id}: Size mismatch - Largest: {largest_size/1024/1024:.1f}MB, Sum others: {sum_others_size/1024/1024:.1f}MB (ratio: {size_ratio:.3f})\")\n",
        "    \n",
        "    if files_to_remove:\n",
        "        result_df = pd.DataFrame(files_to_remove)\n",
        "        total_size_gb = result_df['Size'].sum() / (1024**3)\n",
        "        \n",
        "        print(f\"\\nFound {concatenated_found} DropIDs with concatenated files\")\n",
        "        print(f\"Total files to remove: {len(result_df)}\")\n",
        "        print(f\"Total size to be removed: {total_size_gb:.2f} GB\")\n",
        "        \n",
        "        # Show examples\n",
        "        print(\"\\nExample DropIDs with files to remove:\")\n",
        "        for drop_id in result_df['DropID'].unique()[:3]:\n",
        "            files = result_df[result_df['DropID'] == drop_id]['fileName'].tolist()\n",
        "            sizes = result_df[result_df['DropID'] == drop_id]['Size'].tolist()\n",
        "            print(f\"  {drop_id}: {len(files)} files, {sum(sizes)/1024/1024:.1f}MB total\")\n",
        "            for file, size in zip(files[:3], sizes[:3]):  # Show first 3 files\n",
        "                print(f\"    - {file} ({size/1024/1024:.1f}MB)\")\n",
        "        \n",
        "        return result_df\n",
        "    else:\n",
        "        print(\"No files found for removal (no concatenated files detected)\")\n",
        "        return pd.DataFrame()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get individual files that can be removed (concatenated version exists)\n",
        "files_to_remove = find_already_concatenated_movies_df(movies_df, size_tolerance=0.01)\n",
        "\n",
        "\n",
        "# Remove redundant files\n",
        "for _, row in files_to_remove.iterrows():\n",
        "    print(f\"Safe to remove: {row['Key']} ({row['Size']/1024/1024:.1f}MB)\")\n",
        "    # s3_client.delete_object(Bucket=bucket, Key=row['Key'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPWNn5s5FtwbB065Ta3vhnh",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
