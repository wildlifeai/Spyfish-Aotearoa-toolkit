{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Extract annotation data from Video analysis annotation files.\n",
    "\n",
    "This notebooks is part of the Spyfish Aotearoa existing data cleaning effort and is used to extract the annotations - specifically the surveyIDs, scientific names, (max) count, time of max and time interval - from the files containing video analysis.\n",
    "\n",
    "These annotations are provided by experts. As part of Spyfish Aotearoa, there exist also citizen science annotations and ML annotations (the ML will be trained with the existing expert and cit science annotations).\n",
    "\n",
    "This notebook guides through the export of the annotations from the video analysis files, while also checking the validity and cleaning the various entries. \n",
    "\n",
    "\n",
    "What's happening in this notebook:\n",
    "- load file/relevant excel sheet into df      \n",
    "- clean scientific names\n",
    "- review siteID\n",
    "- define ReplicateWithinSite\n",
    "- create DropID\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last changed 2025.05.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When reviewing this notebook next time, take a look at the Gemini comments here: \n",
    "# https://github.com/wildlifeai/Spyfish-Aotearoa-toolkit/pull/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sftk.s3_handler import S3Handler\n",
    "from sftk.common import S3_SHAREPOINT_SPECIES_CSV, S3_BUCKET, S3_SHAREPOINT_SITE_CSV, S3_SHAREPOINT_SURVEY_CSV\n",
    "from sftk.utils import filter_file_paths_by_extension, read_file_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Scientific names file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_handler = S3Handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_names_df = s3_handler.read_df_from_s3_csv(S3_SHAREPOINT_SPECIES_CSV, S3_BUCKET)\n",
    "scientific_names_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id_df = s3_handler.read_df_from_s3_csv(S3_SHAREPOINT_SITE_CSV, S3_BUCKET)\n",
    "site_id_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df = s3_handler.read_df_from_s3_csv(S3_SHAREPOINT_SURVEY_CSV, S3_BUCKET)\n",
    "surveys_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sharepoint files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and select file to process:\n",
    "\n",
    "1. Download the files from General > Spyfish > Video Analyisis\n",
    "[link](https://docnz.sharepoint.com/teams/SpyfishAotearoa/Shared%20Documents/Forms/AllItems.aspx?id=%2Fteams%2FSpyfishAotearoa%2FShared%20Documents%2FGeneral%2FSpyfish%2FVideo%20analysis)\n",
    "\n",
    "2. Change folder path below `video_analysis_folder`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_analysis_folder = \"/path/to/data/Video analysis\"\n",
    "tabular_file_extensions = (\"xlsx\", \"xls\", \"xlsm\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(video_analysis_folder)\n",
    "tabular_files = filter_file_paths_by_extension(all_files, tabular_file_extensions)\n",
    "tabular_files = [os.path.join(video_analysis_folder, file_name) for file_name in tabular_files if \"~\" not in file_name]  # filters out temporary files]\n",
    "\n",
    "# Enumerate current files and select which one to process next\n",
    "# TODO make the output sorted\n",
    "for i, e in enumerate(tabular_files):\n",
    "    print(i, e.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select file to process by changing number below: \n",
    "\n",
    "The process from here on is to be repeated for each file we need to process.\n",
    "Restart here for each file that needs processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = tabular_files[selected_file]\n",
    "file_name = file_path.split(\"/\")[-1]\n",
    "print(f\"File selected: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data sheet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 'All counts compiled' exists, it will go with it automatically, otherwise select sheet name containing annotations.\n",
    "# DATA is often used, however make sure to check for interval here.\n",
    "\n",
    "sheets = pd.ExcelFile(file_path).sheet_names\n",
    "sheet_name = \"All counts compiled\"\n",
    "if sheet_name not in sheets:\n",
    "    for i, e in enumerate(sheets):\n",
    "        print(i, e)\n",
    "    file_num = int(input(\"select sheet you want to process: \"))\n",
    "    sheet_name = sheets[file_num]\n",
    "    print(\"\\nSelected sheet name: \", sheet_name)\n",
    "\n",
    "current_file_df = read_file_to_df(file_path, sheet_name=sheet_name)\n",
    "print(current_file_df.shape)\n",
    "print(current_file_df.columns)\n",
    "current_file_df.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract & rename the relevant columns to 'ScientificName', 'TimeOfMax', 'MaxInterval'.\n",
    "# TODO test if it works for atypical files\n",
    "# TODO should start interval also be a column\n",
    "\n",
    "output_columns = {'ScientificName', 'TimeOfMax', 'MaxInterval'}\n",
    "columns_in_current_sheet = set(current_file_df.columns)\n",
    "\n",
    "# Different combinations of columns\n",
    "columns_in_sheets = [{'ScientificName', 'TimeOfMaxN', 'MaxN'},\n",
    "           {'ScientificName', 'TimeOfMax', 'MaxN'},\n",
    "           {'ScientificName', 'TimeMaxN', 'MaxN'},\n",
    "           {'ScientificName', 'TimeOfMax', 'MaxInterval'}, \n",
    "           {'ScientificName', 'Timeof MaxN', 'MaxN2'},\n",
    "           {'ScientificNameFish', 'TimeMaxN', 'MaxN'},\n",
    "           {'CommonName', 'TimeOfMax', 'MaxInterval'},\n",
    "           ]\n",
    "\n",
    "if len(output_columns - columns_in_current_sheet) != 0:\n",
    "    for columns in columns_in_sheets:\n",
    "        if len(columns - columns_in_current_sheet) == 0:\n",
    "            print(f\"original column names to be renamed: {columns}\")\n",
    "            rename_dict = dict(zip(columns, output_columns))\n",
    "            current_file_df = current_file_df.rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm/fix SurveyID to match the BUV SurveyID metadata values\n",
    "\n",
    "unique_survey_ids = current_file_df['SurveyID'].unique()\n",
    "print(\"Current file name: \", file_name)\n",
    "print(f\"SurveyIDs in file: {unique_survey_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(surveys_df[\"SurveyID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_survey_ids = set(surveys_df[\"SurveyID\"].unique())\n",
    "\n",
    "survey_mapping = {\n",
    "    'BUV_BNP_20181216' : 'BNP_20181216_BUV',\n",
    "    # MPAMAR Data BUV Akaroa Pohatu 2017 Video analysis data sheet.xlsm\n",
    "    'BUV_BNP_20170223' : 'BNP_20170223_BUV',\n",
    "    #  MPAMAR data Akaroa Pohatu BUV 2021 - Video analysis data sheet - DOC-7166069.xlsm\n",
    "    'BUV_BNP_20210127' : 'BNP_20210127_BUV', \n",
    "    # MPAMAR Data BUV Tonga Island 2021 Video analysis data sheet.xlsm\n",
    "    'BUV_TON_20211026' : 'TON_20211026_BUV', \n",
    "    # MPAMAR Data BUV Te Tapuwae o Rongokako 2021 - Video analysis sheet - DOC-6731514.xlsm\n",
    "    'BUV_TTR_20210125' : 'RON_20210125_BUV',\n",
    "     # MPAMAR Data BUV Tuhua 2021 Video analysis sheet - DOC-6891090.xlsm\n",
    "    'BUV_TUH_20210311' : 'TUH_20210309_BUV',\n",
    "    'BUV_TUH_20210310' : 'TUH_20210309_BUV',\n",
    "    'BUV_TUH_20210309' : 'TUH_20210309_BUV',\n",
    "\n",
    "    # \"MRMDATA - BUV - Parininihi - 2012_2014 - DOC2787054 - DOC-2787054.xlsm\"\n",
    "    'BUV_TNK_20140405': 'PAR_20140404_BUV',  \n",
    "    'BUV_TNK_20120423': 'PAR_20120224_BUV', \n",
    "    # \"MRMDATA - BUV - Tapuae - 2011_2015 - DOC-2639983.xlsm\":\n",
    "    'BUV_TNK_20110324': 'SLI_20110413_BUV',\n",
    "    'BUV_TNK_20130227': 'SLI_20130227_BUV',\n",
    "    'BUV_TNK_20150216': 'SLI_20150216_BUV', \n",
    "    # MPAMAR Data BUV Tonga Island 2021 Video analysis data sheet.xlsm \n",
    "    'BUV_TON_20210101': 'TON_20211026_BUV',\n",
    "    'BUV_TON_20211026': 'TON_20211026_BUV',\n",
    "    'BUV_TON_20211027': 'TON_20211026_BUV',\n",
    "    # MPAMAR Data BUV Tuhua 2020 Video analysis sheet.xlsm\n",
    "    'BUV_TUH_20200922' : \"TUH_20200830_BUV\",\n",
    "    'BUV_TUH_20200831' : \"TUH_20200830_BUV\",\n",
    "    'BUV_TUH_20200901' : \"TUH_20200830_BUV\",\n",
    "    'BUV_TUH_20200830' : \"TUH_20200830_BUV\",\n",
    "    \n",
    "    # MPAMAR Data BUV Horoirangi 2021 - Video analysis sheet.xlsm\n",
    "    'HMR_20211122_BUV' : 'HOR_20211122_BUV',\n",
    "\n",
    "    'BUV_TEA_20210313' : 'ANG_20210313_BUV',\n",
    "    'BUV_TUH_20200922' : 'TUH_20200830_BUV',\n",
    "       \n",
    "    'BUV_TAP_01012022' : 'SLI_20220228_BUV',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_survey_id(survey_id):\n",
    "    if pd.isna(survey_id):\n",
    "        print(\"survey issue\")\n",
    "        return \"FIX\"\n",
    "\n",
    "    if file_name == \"MPAMAR Data BUV Tapuae 2024 - Video analysis sheet.xlsm\":\n",
    "        survey_id = 'SLI_20240124_BUV' \n",
    "\n",
    "    curr_survey = survey_mapping.get(survey_id, survey_id)\n",
    "    if curr_survey not in existing_survey_ids:\n",
    "        print(f\"survey issue,{curr_survey}, {survey_id}\")\n",
    "        return f\"FIX_{curr_survey}\"\n",
    "    \n",
    "    return curr_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm/fix SurveyID to match the BUV SurveyID metadata values\n",
    "unique_survey_ids = current_file_df['SurveyID'].unique()\n",
    "\n",
    "print(\"Current file name: \", file_name)\n",
    "print(f\"SurveyIDs in file: {unique_survey_ids}\")\n",
    "\n",
    "current_file_df['SurveyID'] = current_file_df['SurveyID'].apply(fix_survey_id)\n",
    "\n",
    "print(\"Validated survey ids:\", current_file_df['SurveyID'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you need to fix SiteID, if it looks good go to next section\n",
    "print(f\"row nums: {current_file_df.shape[0]}\")\n",
    "print(f\"len siteIDs: {len(current_file_df['SiteID'].unique())}, nan values: {current_file_df['SiteID'].isna().sum()}\")\n",
    "print(current_file_df['SiteID'].unique())\n",
    "print(f\"len SiteName: {len(current_file_df['SiteName'].unique())}, nan values: {current_file_df['SiteName'].isna().sum()}\")\n",
    "print(current_file_df['SiteName'].unique())\n",
    "print(f\"len SiteCode: {len(current_file_df['SiteCode'].unique())}, nan values: {current_file_df['SiteCode'].isna().sum()}\")\n",
    "print(current_file_df['SiteCode'].unique())\n",
    "print(f\"len SurveyName: {len(current_file_df['SurveyName'].unique())}, nan values: {current_file_df['SurveyName'].isna().sum()}\")\n",
    "print(current_file_df['SurveyName'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO so that it matches survey?\n",
    "# TODO not robust, build on the go, does not cover all cases\n",
    "\n",
    "def fix_site_id(site_id):\n",
    "    # TODO could just add zfill \n",
    "    if site_id == \"BNP_97\":\n",
    "        return \"BNP_097\"\n",
    "    if site_id.startswith(\"TNK\"):\n",
    "        return \"SLI\" + site_id[3:]\n",
    "    return site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file_df[\"SiteID\"] = current_file_df[\"SiteID\"].apply(fix_site_id)\n",
    "print(current_file_df['SiteID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file_df.loc[current_file_df['SiteID'] == \"BNP_97\", 'SiteID'] = \"BNP_097\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file_df[current_file_df[\"SiteID\"] == \"TNK_011\"][[\"Latitude\", \"Longitude\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id_df[site_id_df[\"SiteID\"] == \"SLI_011\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DropID\n",
    "\n",
    "SurveyID_SiteID_ReplicateWithinSite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get replicate withing Site\n",
    "\n",
    "ReplicateWithinSite is a 2 digit number, starting with 1 and for each repetition is a new replicate.\n",
    "\n",
    "If there are multiple years per file, repeat this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_df = current_file_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiteID checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_file_df[ \"SiteID\"] = current_file_df[ \"SiteName\"]\n",
    "\n",
    "# if current_file_df[\"SurveyID\"].unique() == [\"RON_20210125_BUV\"]:\n",
    "#     current_file_df[ \"SiteID\"] = current_file_df[ \"SiteCode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(clean_df))\n",
    "\n",
    "# ['PAR_20120224_BUV' 'PAR_20140404_BUV']\n",
    "# clean_df = clean_df[clean_df['SurveyID'] == 'PAR_20120224_BUV']\n",
    "# clean_df = clean_df[clean_df['SurveyID'] == 'PAR_20140404_BUV']\n",
    "\n",
    "# ['SLI_20110413_BUV', 'SLI_20130227_BUV' , 'SLI_20150216_BUV'  ]\n",
    "# clean_df = clean_df[clean_df['SurveyID'] == 'SLI_20110413_BUV']\n",
    "# clean_df = clean_df[clean_df['SurveyID'] == 'SLI_20130227_BUV']\n",
    "# clean_df = clean_df[clean_df['SurveyID'] == 'SLI_20150216_BUV']\n",
    "\n",
    "print(len(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"SiteIDs with bad deployments: \", clean_df[clean_df['IsBadDeployment'] == True][\"SiteID\"].unique())\n",
    "print(\"Unique SiteIDs with Bad Deployments:\", len(clean_df[clean_df['IsBadDeployment'] == True][\"SiteID\"].unique()))\n",
    "print(\"Total bad deployments:\", len(clean_df[clean_df['IsBadDeployment'] == True][\"SiteID\"]))\n",
    "clean_df[clean_df['IsBadDeployment'] == True][[\"SurveyName\", \"SiteID\", \"SiteName\", \"SiteCode\"]] # ,\"deployment_number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check bad deployments\n",
    "clean_df[clean_df['SiteID'] == \"TON_007\"]\n",
    "clean_df[clean_df[\"SiteID\"] == \"TUH_009\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[clean_df['TimeOfMax'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is a good deployment out of the bad deployment\n",
    "for i in clean_df[clean_df['IsBadDeployment'] == True][\"SiteID\"]:\n",
    "    print(len(clean_df[clean_df[ 'SiteID']== i]), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check things\n",
    "clean_df[clean_df['SiteID']== \"SLI_072\"]\n",
    "site_id_df[site_id_df[\"SiteCode\"] == \"SEPPT_D1\"]\n",
    "site_id_df[site_id_df[\"SiteID\"] == \"TON_006\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ReplicateWithinSite equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(clean_df[\"IsBadDeployment\"] == True))\n",
    "print(sum(clean_df[\"IsNullSample\"] == True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "duplicate = {}\n",
    "clean_df[\"deployment_number\"] = 0\n",
    "for row_id, row in clean_df.iterrows():\n",
    "    curr = duplicate.get(row[\"SiteID\"], 1)\n",
    "    clean_df.at[row_id, \"deployment_number\"] = curr\n",
    "    if row[\"IsBadDeployment\"]:\n",
    "        duplicate[row[\"SiteID\"]] = curr + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"deployment_number\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DropIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_DropID(row):\n",
    "     return f'{row[\"SurveyID\"]}_{row[\"SiteID\"]}_{int(row[\"deployment_number\"]):02d}'\n",
    "\n",
    "clean_df[\"DropID\"] = clean_df.apply(make_new_DropID, axis=1)\n",
    "clean_df[\"DropID\"].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df[clean_df['SiteID']== \"SLI_072\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Scientific Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file_df[current_file_df[\"ScientificName\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New species/new nomenclature/typos found in files matched to Scientific names\n",
    "\n",
    "dict_added = {}\n",
    "\n",
    "# New species\n",
    "dict_added[\"Cheilodactylus spectabilis\"] = \"Chirodactylus spectabilis\"\n",
    "dict_added[\"Pseudolabrus miles\"] = \"Pseudolabrus miles\"\n",
    "dict_added[\"Conger wilsoni\"] = \"Conger wilsoni\"\n",
    "dict_added[\"Pseudocaranx georgianus\"] = \"Pseudocaranx georgianus\"\n",
    "dict_added[\"Chelidonichthys cuculus\"] = \"Chelidonichthys kumu\"\n",
    "\n",
    "\n",
    "# Not accpted anymore\n",
    "dict_added[\"Cephaloscyllium isabellum\"] = \"Cephaloscyllium isabella\"\n",
    "dict_added[\"Chromis dispilus\"] = \"Chromis dispila\"\n",
    "dict_added[\"Upeneichthys porosus\"] = \"Upeneichthys lineatus\"\n",
    "dict_added[\"Upeneichthys porsus\"] = \"Upeneichthys lineatus\" # typo?\n",
    "dict_added[\"Pagrus aurastus\"] = \"Pagrus auratus\"\n",
    "dict_added[\"Dasyatis brevicaudata\"] = \"Bathytoshia brevicaudata\"\n",
    "dict_added[\"Octopus maorium\"] = \"Macroctopus maorum\"\n",
    "\n",
    "\n",
    "# Typos\n",
    "dict_added[\"Pseduolabrus miles\"] = \"Pseudolabrus miles\"\n",
    "dict_added[\"Psedolabrus miles\"] = \"Pseudolabrus miles\"\n",
    "dict_added[\"Odax pullas\"] = \"Odax pullus\"\n",
    "dict_added[\"Chiroremus marmoratus\"] = \"Chironemus marmoratus\"\n",
    "dict_added[\"Psedudocaranx georgianus\"] = \"Pseudocaranx georgianus\"\n",
    "dict_added[\"Gymnothoraz numilus\"] = \"Gymnothorax nubilus\"\n",
    "dict_added[\"Psuedophyscis bacchus\"] = \"Pseudophycis bachus\"\n",
    "dict_added[\"Psuedophycis bachus\"] = \"Pseudophycis bachus\"\n",
    "dict_added[\"Parus auratus\"] = \"Pagrus auratus\"\n",
    "dict_added[\"Chrysophrys auratus\"] = \"Pagrus auratus\"\n",
    "dict_added[\"Chelidonchthys kumu\"] = \"Chelidonichthys kumu\"\n",
    "dict_added[\"Paraperis colias\"] = \"Parapercis colias\"\n",
    "dict_added[\"Seriola lalandi lalandi\"] = \"Seriola lalandi\"\n",
    "\n",
    "# Too broad, to fix\n",
    "dict_added[\"Chondrichthyes\"] = \"_FIXChondrichthyes\"\n",
    "\n",
    "# To Check\n",
    "# didn't exist\n",
    "dict_added[\"Notoclinus cinctus\"] = \"Notolabrus cinctus\" # is it Notolabrus cinctus?\n",
    "\n",
    "\n",
    "dict_added[\"Oligoplites saurus\"] = \"Oligoplites saurus\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_added[\"Zearaja nasuta\"] = \"Dipturus nasutus\"\n",
    "\n",
    "\n",
    "# dict_scientific_scientific.update(dict_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra dictionary with invalid, null deployments and undefined species (TODO: these are getting fixed)\n",
    "extras_dict = {}\n",
    "# for i in [\"Bad deployment\", 'Null', \"Null sample\"]:\n",
    "#     extras_dict.update({i: i.upper()})\n",
    "# extras_dict[\"Null sample\"] = \"NULL\"\n",
    "for i in [ \"Sp1\", \"Sp2\", \"Sp3\", \"Sp4\", \"Sp5\", \"Sp6\", \"Sp7\", \"Unknown\", \"Other\"]:\n",
    "    extras_dict.update({i: i.lower()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_name_set = set(scientific_names_df[\"scientificName\"])\n",
    "common_name_dict = dict(zip(scientific_names_df[\"commonName\"], scientific_names_df[\"scientificName\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for different name categories\n",
    "sn = 0  # Scientific names found\n",
    "cn = 0  # Common names found\n",
    "dn = 0  # Names found in user-added dictionary\n",
    "nn = 0  # Names not found (require fixing)\n",
    "en = 0  # Names found in extras dictionary\n",
    "nd = 0  # Null or 'Null sample' values\n",
    "bd = 0  # 'Bad deployment' entries\n",
    "def clean_name(name):\n",
    "    global sn, cn, dn, nn, en, nd, bd\n",
    "    \n",
    "    name = \" \".join(name.strip().split()).capitalize()\n",
    "\n",
    "    if pd.isna(name):\n",
    "        nn+=1\n",
    "        return \"NULL\"\n",
    "    \n",
    "    if name in scientific_name_set:\n",
    "        sn+=1\n",
    "        return name\n",
    "\n",
    "    if name in dict_added:\n",
    "        dn+=1\n",
    "        return dict_added[name]\n",
    "    \n",
    "    if name in common_name_dict:\n",
    "        cn+=1\n",
    "        return common_name_dict[name]\n",
    "    \n",
    "    if name in { \"Sp1\", \"Sp2\", \"Sp3\", \"Sp4\", \"Sp5\", \"Sp6\", \"Sp7\", \"Unknown\", \"Other\"}:\n",
    "        en +=1\n",
    "        return name.lower()\n",
    "    \n",
    "    if name in {'Null', \"Null sample\"}:\n",
    "        nd +=1\n",
    "        return \"NULL\"\n",
    "    \n",
    "    if name == \"Bad deployment\":\n",
    "        bd+=1\n",
    "        return \"BAD DEPLOYMENT\"\n",
    "\n",
    "    print(\"name not found\", name)\n",
    "    nn+=1\n",
    "    return f\"FIX_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print(file_name)\n",
    "clean_df[\"ScientificName\"] = clean_df[\"ScientificName\"].apply(clean_name)\n",
    "# Print summary of results\n",
    "print(f\"\"\"Summary of name cleaning:\n",
    "- Scientific names found        : {sn}\n",
    "- Dictionary-added names        : {dn}\n",
    "- Common names found            : {cn}\n",
    "- Extras (sp, unknown) found    : {en}\n",
    "- Null/Null sample entries      : {nd}\n",
    "- Bad deployment entries        : {bd}\n",
    "- Names not found (FIX_...)     : {nn}\n",
    "\"\"\")\n",
    "\n",
    "np.sort(clean_df[\"ScientificName\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check NULL SAMPLE in max and time\n",
    "# TODO check what this does\n",
    "\n",
    "print(sum(clean_df[\"ScientificName\"]== \"NULL SAMPLE\"), \"should be 0\")\n",
    "# current_file_df[\"ScientificName\"][current_file_df[\"ScientificName\"] == \"NULL SAMPLE\"] = \"NULL\"\n",
    "\n",
    "# sum(current_file_df[\"ScientificName\"]== \"NULL SAMPLE\"))\n",
    "clean_df['MaxInterval'] = clean_df['MaxInterval'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "print(len(clean_df[clean_df['TimeOfMax']==\"\"]))\n",
    "clean_df['MaxInterval'][clean_df['MaxInterval']== \"NULL SAMPLE\"] = 'NULL'\n",
    "clean_df['MaxInterval'][clean_df['MaxInterval']==\"\"] = 'NULL'\n",
    "print(len(clean_df[clean_df['TimeOfMax']==\"\"]))\n",
    "\n",
    "clean_df['TimeOfMax'] = clean_df['TimeOfMax'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "clean_df['TimeOfMax'][clean_df['TimeOfMax']==\"NULL SAMPLE\"] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bad deployments\n",
    "print(len(clean_df), sum(clean_df[\"ScientificName\"] == 'BAD DEPLOYMENT'))\n",
    "clean_df = clean_df[clean_df[\"IsBadDeployment\"] != True]\n",
    "len(clean_df), sum(clean_df[\"ScientificName\"] == 'BAD DEPLOYMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[clean_df[\"TimeOfMax\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix datetime\n",
    "\n",
    "def fix_datetime(row):\n",
    "    date = row['TimeOfMax']\n",
    "    good = False\n",
    "    while good == False:\n",
    "        \n",
    "        if type(date) == str: \n",
    "            if date == \"NULL\":\n",
    "                good = True\n",
    "            else:\n",
    "                try: \n",
    "                    date = date.replace(\";\", \":\")\n",
    "                    date_list = date.split(\":\")\n",
    "                    seconds = int(date_list[-1])\n",
    "\n",
    "                    mins = int(date_list[-2])\n",
    "\n",
    "                    if len(date_list) == 3:\n",
    "                        hours = int(date_list[0])\n",
    "                    else:\n",
    "                        hours = 0\n",
    "                    print(f'{hours}:{mins}:{seconds}', row[\"DropID\"], row[\"ScientificName\"])\n",
    "\n",
    "                    date = datetime.time(hours,mins,seconds)\n",
    "                    print(date)\n",
    "                    good = True\n",
    "                except Exception as e:\n",
    "                    print(\"what??\", e)\n",
    "                    pass\n",
    "\n",
    "            if not good:\n",
    "                print(\"Current date doesn't fit format: \", date, row[\"DropID\"], row[\"ScientificName\"])\n",
    "                date_str = input(\"Type out the time in following format HH:MM:SS)\") \n",
    "                print( row[\"DropID\"], row[\"ScientificName\"])\n",
    "\n",
    "                date = datetime.datetime.strptime(f'{date_str[:2]}:{date_str[3:5]}:{date_str[6:]}', '%H:%M:%S').time()\n",
    "                good = True\n",
    "            # TODO: check first and second and third pair is digit.\n",
    "        elif isinstance(date, datetime.time):\n",
    "            good = True\n",
    "        else:\n",
    "            print(type(date), date)\n",
    "\n",
    "            date = \"NULL\"            \n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_df['TimeOfMax'].unique()), len(clean_df['MaxInterval'].unique()))\n",
    "\n",
    "clean_df['TimeOfMax'] = clean_df.apply(fix_datetime, axis=1)\n",
    "\n",
    "print(len(clean_df['TimeOfMax'].unique()), len(clean_df['MaxInterval'].unique()))\n",
    "\n",
    "print(clean_df['TimeOfMax'].unique())\n",
    "print(clean_df['MaxInterval'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_file_df[current_file_df[\"SiteID\"] == \"SLI_076\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = clean_df[['DropID','ScientificName', 'TimeOfMax', 'MaxInterval']].copy()\n",
    "selected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export annotation file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check validity of various columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review null deployments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There should be the same amount of nulls...\n",
    "\n",
    "print(len(selected_df[selected_df[\"ScientificName\"] == \"NULL\"]))\n",
    "print(len(selected_df[selected_df[\"TimeOfMax\"] == \"NULL\"]))\n",
    "print(len(selected_df[selected_df[\"MaxInterval\"] == \"NULL\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df[selected_df[\"TimeOfMax\"] == \"NULL\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(selected_df[selected_df[\"ScientificName\"].isna()]))\n",
    "print(len(selected_df[selected_df[\"TimeOfMax\"].isna()]))\n",
    "print(len(selected_df[selected_df[\"MaxInterval\"].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(sum(clean_df[\"ScientificName\"] == \"NULL\"))\n",
    "clean_df['ScientificName'] = clean_df['ScientificName'].fillna('NULL')\n",
    "clean_df['TimeOfMax'] = clean_df['TimeOfMax'].fillna('NULL')\n",
    "clean_df['MaxInterval'] = clean_df['MaxInterval'].fillna('NULL')\n",
    "print(sum(clean_df[\"ScientificName\"] == \"NULL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check species names\n",
    "\n",
    "Species underscored with FIX_ need review, as do sp1, sp2, sp3, sp4, sp5, sp6, sp7, as do any mention of unknown/undefined.\n",
    "\n",
    "TODO:\n",
    "- check species with species name checker to make sure all is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df[\"ScientificName\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_names = selected_df[\"ScientificName\"].unique()\n",
    "for i in np.sort(species_names[1:]):\n",
    "    print(i)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check MaxIterval & TimeOfMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df[\"MaxInterval\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any times that do not follow the predefined format or NULL\n",
    "\n",
    "for row in selected_df[\"TimeOfMax\"]:\n",
    "    if type(row) == str:\n",
    "        if row != \"NULL\":\n",
    "            print(row)\n",
    "    elif type(row) == datetime.time:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: duplicates usually when TimeofMax missing\n",
    "selected_df[selected_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df[selected_df.index == 215]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_df = current_file_df[['DropID','ScientificName', 'TimeOfMax', 'MaxInterval']].copy()\n",
    "selected_df['AnnotatedBy'] = \"expert\"\n",
    "\n",
    "# TODO select the right annotations (should it be an input?)\n",
    "# All Counts Compiled -> IntervalAnnotation 30, \n",
    "# Max Count Compiled more -> IntervalAnnotation 1800,\n",
    "\n",
    "# interval_annotation = 1800\n",
    "interval_annotation = 30\n",
    "selected_df['IntervalAnnotation'] = interval_annotation\n",
    "selected_df = selected_df.fillna(\"NULL\")\n",
    "selected_df['ConfidenceAgreement'] = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveyID = clean_df[\"SurveyID\"].unique()[0]\n",
    "surveyID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_to_annotations(df_with_vals, file_name, selected_folder, export_csv_file_name=None):\n",
    "    # Export extracted annotations to csv sheet in export folder\n",
    "    if not export_csv_file_name:\n",
    "        export_file_name = os.path.splitext(os.path.basename(file_name))[0]\n",
    "        export_csv_file_name = f\"annotations_buv__{interval_annotation}__{surveyID}__{export_file_name}.csv\"\n",
    " \n",
    "    # create export folder in folder containing the annotation files \n",
    "    # TODO check this\n",
    "    path_to_export = os.path.join(selected_folder, \"export\")\n",
    "    os.makedirs(path_to_export, exist_ok=True)\n",
    "    export_location = os.path.join(path_to_export, export_csv_file_name)\n",
    "    print(f\"Exporting data to file: '{export_location}'\")\n",
    "    df_with_vals.to_csv(export_location,index=False)  \n",
    "    \n",
    "print(f\"Showing sample of export with shape: {selected_df.shape}\")\n",
    "display(selected_df.sample(10))\n",
    "export_to_annotations(selected_df, file_name, video_analysis_folder)     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyfish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
